{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c3dd3-9597-4731-9051-340815c9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e02c34-a721-4728-a8f5-22efd2f5209e",
   "metadata": {},
   "source": [
    "### __1. Import and data pre-processing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc798d16-acd6-4eb6-a9f9-2bfd01d7b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import counts dataframe:\n",
    "\n",
    "df_counts = pd.read_csv(\"dataset/mdd_counts.csv\") \n",
    "df_counts = df_counts.set_index(\"Unnamed: 0\")\n",
    "\n",
    "print(df_counts.shape)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797da98-2b49-4da3-bdde-dfe7e6a5af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metadata:\n",
    "\n",
    "df_meta = pd.read_csv(\"dataset/mdd_meta.csv\")\n",
    "df_meta = df_meta.set_index(\"!Sample_title\")\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11561eb-56b5-4129-9c2b-d8f8f3cd5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select (x, y) data:\n",
    "# x -> counts table\n",
    "# y -> (sample, phenotype, denger)\n",
    "\n",
    "counts = df_counts.to_numpy()\n",
    "print(counts.min(), counts.mean(), counts.max())\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.imshow(counts)\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7f8ec-b57f-4050-b670-c0fb758c4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization:\n",
    "x = np.log(counts+1)\n",
    "x /= x.max()\n",
    "\n",
    "print(x.min(), x.mean(), x.max())\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.imshow(x)\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1249846-21b4-4fd3-9c84-bb6ac848763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta -> (subject_id, sample, phenotype, gender)\n",
    "\n",
    "meta = [x.split(\".\") for x in df_counts.index]\n",
    "\n",
    "N = len(meta)\n",
    "\n",
    "for i in range(N):\n",
    "    n = meta[i][0]\n",
    "    \n",
    "    meta[i] += df_meta.loc[['phenotype', 'gender'], n].tolist()\n",
    "\n",
    "meta = np.array(meta)\n",
    "print(meta.shape)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ada874-95bd-48fc-a014-6f724ce5ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding:\n",
    "\n",
    "def one_hot(meta):\n",
    "\n",
    "    labels = np.array(list(set(meta)))\n",
    "    hot = np.zeros((len(meta), len(labels)))\n",
    "\n",
    "    for i, x in enumerate(meta):\n",
    "        j = np.where(x == labels)[0]\n",
    "        hot[i][j] = 1.0\n",
    "\n",
    "    return hot\n",
    "\n",
    "y1 = one_hot(meta.T[1])\n",
    "y2 = one_hot(meta.T[2])\n",
    "y3 = one_hot(meta.T[3])\n",
    "\n",
    "print(f\"y1:{y1.shape}, y2:{y2.shape}, y3:{y3.shape}\")\n",
    "\n",
    "y1[:2], y2[:2], y3[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803f14c-e0b4-4288-adca-105d6eabd00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling:\n",
    "N_samples = x.shape[0]\n",
    "i = np.random.permutation(N_samples)\n",
    "\n",
    "x, y1, y2, y3 = x[i], y1[i], y2[i], y3[i]\n",
    "meta = meta[i]\n",
    "\n",
    "x.shape, y1.shape, y2.shape, y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb7c35-c817-40ef-85e3-89c36050ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add channel for convnet:\n",
    "x = np.expand_dims(x, axis=-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ec415-41ec-4d84-ac7d-9043a60a01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting (x, y) into (x, x-test):\n",
    "N_test = int(0.10*N_samples)\n",
    "\n",
    "x_test = x[:N_test]\n",
    "x = x[N_test:]\n",
    "\n",
    "print(f\"x:{x.shape}, x-test:{x_test.shape}\")\n",
    "\n",
    "y1_test = y1[:N_test]\n",
    "y1 = y1[N_test:]\n",
    "\n",
    "print(f\"y1:{y1.shape}, y1-test:{y1_test.shape}\")\n",
    "\n",
    "y2_test = y2[:N_test]\n",
    "y2 = y2[N_test:]\n",
    "\n",
    "print(f\"y2:{y2.shape}, y2-test:{y2_test.shape}\")\n",
    "\n",
    "y3_test = y3[:N_test]\n",
    "y3 = y3[N_test:]\n",
    "\n",
    "print(f\"y3:{y3.shape}, y3-test:{y3_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34954bb5-adc1-403e-a344-881c8814b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting unnecessary arrays:\n",
    "del df_counts, df_meta, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297d543-bb47-455f-9cc3-0e129221f760",
   "metadata": {},
   "source": [
    "### __2. Neural network modeling:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9009d9d-a1c8-4a85-b0df-242454ff597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyModel(latent_dim=20, act=\"relu\", kernel_size=5):\n",
    "    \n",
    "    input_size = 1052\n",
    "    \n",
    "    In = keras.Input((input_size, 1))\n",
    "    \n",
    "    x = layers.Conv1D(32, kernel_size, activation=act, padding=\"same\")(In)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, kernel_size, activation=act, padding=\"same\")(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Dense(latent_dim)(x)\n",
    "    x = layers.BatchNormalization()(x) \n",
    "    x = layers.Activation(\"tanh\", name=\"ls\")(x)\n",
    "    \n",
    "    x = layers.Dense(263, activation=act)(x)\n",
    "\n",
    "    x = layers.Dense(263*64, activation=act)(x)\n",
    "    x = layers.Reshape((263, 64))(x)\n",
    "    \n",
    "    x = layers.Conv1DTranspose(64, kernel_size, activation=act, strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv1DTranspose(32, kernel_size, activation=act, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    Out = layers.Conv1D(1, kernel_size, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    \n",
    "    return keras.Model(inputs=In, outputs=Out, name='autoencoder')\n",
    "\n",
    "latent_dim = 2\n",
    "model = MyModel(latent_dim=latent_dim, act=\"elu\", kernel_size=5)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbef323-3366-407b-a159-8f4da705f9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dff06e-df41-40fb-b520-ff71f777881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8aa8f80-0c31-4ad1-835f-06fd72550a76",
   "metadata": {},
   "source": [
    "### __3. Model compilation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6122e-eba6-4c60-92f7-81ec0beb8aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aec05f-a9eb-4e1f-b51f-5722f8815016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beda4521-0d6c-42fe-995f-dd52c4ae9bc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __4. K-fold cross-validation:__   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6438357b-6b49-4a4d-9643-ad55e186f313",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "\n",
    "K = 3\n",
    "N_val = len(x)//K\n",
    "k_hist = []\n",
    "\n",
    "for k in range(K):\n",
    "    print(f\"\\n#{k+1}-fold:\", end=\"\")\n",
    "    print(\"\".join(110*['_']), \"\\n\")\n",
    "    \n",
    "    # 1. splitting the data into train and validation:\n",
    "    x_val = x[k*N_val:(k+1)*N_val]\n",
    "    x_train = np.concatenate([x[:k*N_val], x[(k+1)*N_val:]])\n",
    "\n",
    "    \n",
    "    y1_val = y1[k*N_val:(k+1)*N_val]\n",
    "    y1_train = np.concatenate([y1[:k*N_val], y1[(k+1)*N_val:]])\n",
    "\n",
    "    y2_val = y2[k*N_val:(k+1)*N_val]\n",
    "    y2_train = np.concatenate([y2[:k*N_val], y2[(k+1)*N_val:]])\n",
    "\n",
    "    y3_val = y3[k*N_val:(k+1)*N_val]\n",
    "    y3_train = np.concatenate([y3[:k*N_val], y3[(k+1)*N_val:]])\n",
    "\n",
    "    y_train = [y1_train, y2_train, y3_train]\n",
    "    y_val = [y1_val, y2_val, y3_val]\n",
    "    \n",
    "    \n",
    "    # 2. reset model:\n",
    "    model, _, __ = MyModel(params_dim=params_dim, act=\"elu\", kernel_size=5)\n",
    "    \n",
    "    model.compile(optimizer=\"Adam\", loss=3*[\"categorical_crossentropy\"]+[\"mse\"], \n",
    "                  metrics=3*[\"acc\"]+[\"mae\"])\n",
    "    \n",
    "    # 3. fittting the model:\n",
    "    report = model.fit(x=[x_train, y_train], \n",
    "                       y=[y_train, x_train], \n",
    "                       validation_data=[[x_val, y_val], [y_val, x_val]],\n",
    "                       epochs=20, batch_size=1)\n",
    "\n",
    "    # 4. recording performance:\n",
    "    k_hist.append(report.history)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e2b1b25-52ac-44de-8b45-21ac52f5d287",
   "metadata": {},
   "source": [
    "n_epochs = len(k_hist[0]['loss'])\n",
    "epochs = np.linspace(1, n_epochs, n_epochs)\n",
    "\n",
    "k_val_ls1_acc = []\n",
    "k_val_ls2_acc = []\n",
    "k_val_ls3_acc = []\n",
    "k_val_x_hat_mae = []\n",
    "\n",
    "for k, hist in enumerate(k_hist, start=1):\n",
    "\n",
    "    loss = hist['loss']\n",
    "    val_loss = hist['val_loss']\n",
    "\n",
    "    ls1_acc = hist['ls1_acc']\n",
    "    ls2_acc = hist['ls2_acc']\n",
    "    ls3_acc = hist['ls3_acc']\n",
    "    \n",
    "    val_ls1_acc = hist['val_ls1_acc']\n",
    "    val_ls2_acc = hist['val_ls2_acc']\n",
    "    val_ls3_acc = hist['val_ls3_acc']\n",
    "\n",
    "    x_hat_mae = hist['x_hat_mae']\n",
    "    val_x_hat_mae = hist['val_x_hat_mae']\n",
    "    \n",
    "    k_val_ls1_acc.append(val_ls1_acc)\n",
    "    k_val_ls2_acc.append(val_ls2_acc)\n",
    "    k_val_ls3_acc.append(val_ls3_acc)\n",
    "    k_val_x_hat_mae.append(val_x_hat_mae)\n",
    "\n",
    "    print(f\"#{k}-epoch: <loss> = {np.mean(loss):.3f}, <val-loss> = {np.mean(val_loss):.3f}\", end=\", \")\n",
    "    print(f\"<ls1-acc> = {np.mean(ls1_acc):.3f}, <val-ls1-acc> = {np.mean(val_ls1_acc):.3f}\")\n",
    "    print(f\"<ls2-acc> = {np.mean(ls2_acc):.3f}, <val-ls2-acc> = {np.mean(val_ls2_acc):.3f}\")\n",
    "    print(f\"<ls3-acc> = {np.mean(ls3_acc):.3f}, <val-ls3-acc> = {np.mean(val_ls3_acc):.3f}\")\n",
    "    print(f\"<x-hat-mae> = {np.mean(x_hat_mae):.3f}, <val-ls-mae> = {np.mean(val_x_hat_mae):.3f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(21, 4))\n",
    "    \n",
    "    ax[0].plot(epochs, loss, label=\"loss\")\n",
    "    ax[0].plot(epochs, val_loss, label=\"val_loss\")\n",
    "    \n",
    "    ax[1].plot(epochs, x_hat_mae, label=\"x-hat-mae\")\n",
    "    ax[1].plot(epochs, val_x_hat_mae, label=\"val-x-hat-mae\")\n",
    "    \n",
    "    for i in range(2):\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].legend()\n",
    "        ax[i].grid()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(21, 4))\n",
    "    \n",
    "    ax[0].plot(epochs, ls1_acc, label=\"ls1-acc\")\n",
    "    ax[0].plot(epochs, val_ls1_acc, label=\"val_ls1-acc\")\n",
    "\n",
    "    ax[1].plot(epochs, ls2_acc, label=\"ls2-acc\")\n",
    "    ax[1].plot(epochs, val_ls2_acc, label=\"val_ls2-acc\")\n",
    "\n",
    "    ax[2].plot(epochs, ls3_acc, label=\"ls3-acc\")\n",
    "    ax[2].plot(epochs, val_ls3_acc, label=\"val_ls3-acc\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].legend()\n",
    "        ax[i].grid()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(f\"Validation ls1-acc over k-fold: ({np.mean(k_val_ls1_acc):.3f} +- {np.std(np.mean(k_val_ls1_acc, axis=1)):.4f})\")\n",
    "print(f\"Validation ls2-acc over k-fold: ({np.mean(k_val_ls2_acc):.3f} +- {np.std(np.mean(k_val_ls2_acc, axis=1)):.4f})\")\n",
    "print(f\"Validation ls3-acc over k-fold: ({np.mean(k_val_ls3_acc):.3f} +- {np.std(np.mean(k_val_ls3_acc, axis=1)):.4f})\")\n",
    "print(f\"Validation x-hat-MAE over k-fold: ({np.mean(k_val_x_hat_mae):.3f} +- {np.std(np.mean(k_val_x_hat_mae, axis=1)):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8686e6-36cf-414e-8fed-238a7c2024ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __5. Final training__    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "95a01d62-07ab-4173-b5f7-3fe64e80d442",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "report = model.fit(x=[x, [y1, y2, y3]], y=[[y1, y2, y3], x], epochs=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "080fec86-c546-4e37-9840-7de22dc5dddd",
   "metadata": {},
   "source": [
    "loss = report.history['loss']\n",
    "ls1_acc = report.history['ls1_acc']\n",
    "ls2_acc = report.history['ls2_acc']\n",
    "ls3_acc = report.history['ls3_acc']\n",
    "x_hat_mae = report.history['x_hat_mae']\n",
    "\n",
    "epochs = np.linspace(1, len(loss), len(loss))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,4))\n",
    "\n",
    "ax[0].plot(epochs, loss, label=\"loss\")\n",
    "ax[1].plot(epochs, x_hat_mae, label=\"x-hat-mae\")\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20,4))\n",
    "\n",
    "ax[0].plot(epochs, ls1_acc, label=\"ls1_acc\")\n",
    "ax[1].plot(epochs, ls2_acc, label=\"ls2_acc\")\n",
    "ax[2].plot(epochs, ls3_acc, label=\"ls3_acc\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e875dc-7d57-42fb-a03b-19948e2de183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __6. Test evaluation__:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f13d1470-0c54-4ef4-9de8-670eafa84dfc",
   "metadata": {},
   "source": [
    "model.evaluate(x=[x_test, [y1_test, y2_test, y3_test]], \n",
    "               y=[[y1_test, y2_test, y3_test], x_test], \n",
    "               batch_size=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "296facde-2463-4cb9-942c-1bfab080bce9",
   "metadata": {},
   "source": [
    "x = np.concatenate([x_test, x])\n",
    "\n",
    "y1 = np.concatenate([y1_test, y1])\n",
    "y2 = np.concatenate([y2_test, y2])\n",
    "y3 = np.concatenate([y3_test, y3])\n",
    "\n",
    "y_pred, x_pred = model.predict([x, [y1, y2, y3]])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 6))\n",
    "\n",
    "ax[0].imshow(x)\n",
    "ax[1].imshow(x_pred)\n",
    "\n",
    "for i in range(2):        \n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e09c96-8a0b-4541-969f-d2a35bfb1622",
   "metadata": {},
   "source": [
    "#### __6.1 Anomaly detection:__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "442a9d7c-32f2-431e-b733-2e75cd3dfa50",
   "metadata": {},
   "source": [
    "mse = np.mean(pow(x-x_pred, 2), axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17464276-41a7-4a5a-9b42-cd7228fdc799",
   "metadata": {},
   "source": [
    "threshold = 0.002\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.hist(mse, bins=50, histtype=\"step\", lw=2)\n",
    "plt.axvline(x=threshold, color='pink')\n",
    "plt.xlabel(\"MSE\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()\n",
    "\n",
    "j = np.where(mse >= threshold)[0]\n",
    "\n",
    "print(\"Outliers:\\n\", meta[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cb835-de40-4897-869d-ffc9e6c0a6fc",
   "metadata": {},
   "source": [
    "#### __6.2 Classification:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b91e54-e0c7-43a3-a880-ebb988c47a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2cb653-20ae-47ce-bcad-b3d9777584f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2129ea95-31f4-4efd-b7ae-afe18eda2483",
   "metadata": {},
   "source": [
    "#### __6.3 Data generator:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9a9f9-4eab-42c8-bc5e-bcc24551a68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949d44e-ba85-4b0c-ab9b-8e6981274869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1006c9-50c7-4341-a265-a9221ae1ebca",
   "metadata": {},
   "source": [
    "### __7. Saving the model__:\n",
    "<font size=3>\n",
    "    \n",
    "For model __loading__, see [2.2-notebook](2.2-notebook.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
