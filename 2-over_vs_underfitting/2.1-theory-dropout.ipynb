{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bef94e-ed7a-45c6-925e-d255048ca871",
   "metadata": {},
   "source": [
    "### __1. Underfitting and overfitting:__\n",
    "<font size=3>\n",
    "\n",
    "During the __train-validation step__, using the _training data_, the _optimizer_ updates the values of the model's inner parameters (_i.e._, weights, biases, etc.) over the epochs while minimizing/maximizing the loss function. Meanwhile, the model's performance is measured for each epoch using the validation data. At this workflow stage, we model the neural network architecture to avoid [overfitting and underfitting](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/).\n",
    "\n",
    "__Underfitting__ means a poor NN fitting, _i.e._, the model does not learn well. On the other hand, __overfitting__ occurs when the model fits the training data very well but makes poor predictions with validation data.\n",
    "\n",
    "__To avoid underfitting__, we need to make the NN more robust - with more layers and neurons - to increase the NN's depth.\n",
    "\n",
    "__To avoid overfitting__, we have two basic options: __i)__ decrease the number of neurons (or/and layers) - as an analogy, we are decreasing the degree of a polynomial function (check the [figure](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/) again); __ii)__ we can apply a dropout to a layer with large number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53d2f6-17fc-425b-9851-30ed2c0bb701",
   "metadata": {},
   "source": [
    "### __2. The Dropout:__\n",
    "<font size=3>\n",
    "\n",
    "What is the dropout layer? Dropout _\"closes\"_ the activation of neurons from the previous layer at random by setting them to zero! When training becomes rigid, we create a type of _\"neuroplasticity\"_ in the network to form more flexible connections. \n",
    "\n",
    "The figure below, from the pioneer [paper](https://paperswithcode.com/method/dropout), shows, on the left, an example of an NN with three hidden layers, each with five neurons and an output with a single neuron. On the right, the dropout applied in these three hidden layers illustrates how the neuron connections change (are blocked). The new input -> output connections can make the _pathway_ more flexible, preventing overfitting. Have a look at the paper's motivation section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ae96f-859e-4743-9cac-c037b6305c1f",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../figs/dropout.png\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6749a1b4-3a21-4cc8-8ba4-6a53097455ee",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "In the __keras__ [layers.Dropout(q)](https://keras.io/api/layers/regularization_layers/dropout/) function, we determine the quantile $q$ of neurons from the chosen layer, which will be randomly set to zero. Below, we have a model with three hidden layers. The 2nd hidden layer has a 50% dropout, so 100 neurons will be set to zero.\n",
    "\n",
    "```python\n",
    "In = keras.Input(shape=(x_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(50, activation='sigmoid')(In)\n",
    "\n",
    "x = keras.layers.Dense(200, activation='sigmoid')(x)\n",
    "\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = keras.layers.Dense(20, activation='sigmoid')(x)\n",
    "\n",
    "Out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=In, outputs=Out)\n",
    "```\n",
    "\n",
    "<br>\n",
    "In another example below, we have a model with three hidden layers, with 30% and 40% dropouts in the 2nd and 3rd hidden layers, respectively.\n",
    "\n",
    "```python\n",
    "In = keras.Input(shape=(x_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(150, activation='sigmoid')(In)\n",
    "\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = keras.layers.Dense(80, activation='sigmoid')(x)\n",
    "\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x = keras.layers.Dense(20, activation='sigmoid')(x)\n",
    "\n",
    "Out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=In, outputs=Out)\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb7737-8ba2-44c3-b78f-f1f5b60564aa",
   "metadata": {},
   "source": [
    "#### __2.1 What about the math?__\n",
    "<font size=3>\n",
    "    \n",
    "In the paper, the authors compute the dropout by multiplying the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) $r_l^i$ of probability $p$ in the layer $a_l^i$,\n",
    "\n",
    "\\begin{align}\n",
    "    r_{l-1}^i &\\sim Bernoulli(p) \\, , \\\\\n",
    "    \\tilde a_{l-1}^i &= r_{l-1}^i \\odot a_{l-1}^i \\, , \\\\\n",
    "    a_l^i &= \\sigma_l\\left(W_l^{ij}\\,\\tilde a_{l-1}^j + b_l^i\\right) \\, .\\\\\n",
    "\\end{align}\n",
    "\n",
    "Below, an example how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c6d303-a333-41e7-b13c-6538fe1a8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976fcdb9-6cb9-424c-9ed8-57121a348eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10461083, -0.78009315,  1.11124381,  0.04286497, -1.03652512,\n",
       "       -2.37830578, -0.04181743,  2.0773545 , -0.50668722, -0.22254797,\n",
       "        0.6602887 , -0.55147563,  0.21989477,  0.68738247,  0.27180098,\n",
       "       -1.50879617, -0.94190003, -2.60679257,  0.05628814,  1.33111176])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = randn(20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14de0245-8b1e-4cd6-a33e-6e04e7f8759e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05230541, -0.        ,  1.11124381,  0.02143249, -0.        ,\n",
       "       -0.        , -0.        ,  2.0773545 , -0.        , -0.        ,\n",
       "        0.33014435, -0.        ,  0.10994739,  0.34369124,  0.13590049,\n",
       "       -0.        , -0.        , -0.        ,  0.02814407,  1.33111176])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.5\n",
    "\n",
    "r = bernoulli.cdf(a, p)\n",
    "\n",
    "a = r*a\n",
    "\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
