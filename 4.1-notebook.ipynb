{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8e5cc3-2097-4449-894c-9f3fd1d5dfec",
   "metadata": {},
   "source": [
    "## __The autoencoder architecture__\n",
    "<font size=3>\n",
    "\n",
    "An autoencoder NN consists of two main components: the __encoder__ and the __decoder__. The __encoder__ takes input data $x$ and compresses it through multiple layers (_e.g._, dense, convolutional), reducing it to a smaller representation known as the __latent-space__ $\\hat y$. This smaller layer captures the essential features of the data, allowing the feature information to flow from input $x$ to output $\\hat x$. The __decoder__ then takes the latent-space representation $\\hat y$ and reconstructs it into a form resembling the original data $\\hat x$. The figure below illustrates this architecture.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/autoencoder.png\" width=\"450\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bec63-ff61-4331-8b16-a78bdc8aa1c8",
   "metadata": {},
   "source": [
    "#### __Unsupervised learning:__\n",
    "<font size=3>\n",
    "\n",
    "Autoencoders are often used in unsupervised learning tasks, where the dataset $(x)$ consists solely of the input data without labels or direct correlations. Common applications include __anomaly detection__, __noise reduction__, and __data segmentation__, in which the training minimizes the loss function $\\mathcal{L}(x,\\, \\hat x)$.\n",
    "\n",
    "The latent-space, generated by the encoder, can also be used for __dimensionality reduction__, where the combined size of the model script and parameters is smaller than the original dataset. Additionally, the latent-space can serve as a data map for __clustering__ tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bac052-9d94-4fd1-8525-2705f2828f52",
   "metadata": {},
   "source": [
    "#### __Supervised learning:__\n",
    "<font size=3>\n",
    "    \n",
    "In supervised learning $(x,\\, y)$, the latent-space $\\hat y$ can be leveraged to minimize the loss function $\\mathcal{L}(y,\\,\\hat y)$, allowing autoencoder NNs to perform tasks such as __classification__ and __anomaly detection__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
