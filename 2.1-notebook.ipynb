{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c3dd3-9597-4731-9051-340815c9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba7112-8800-452f-8614-0908a99637a7",
   "metadata": {},
   "source": [
    "### __Deep Learning workflow:__\n",
    "<font size=3>\n",
    "    \n",
    "1. Import and data pre-processing;   \n",
    "2. Neural network modeling;\n",
    "3. Model compilation;\n",
    "4. Train and validation;\n",
    "5. Final training;\n",
    "6. Test evaluation;\n",
    "7. Saving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e02c34-a721-4728-a8f5-22efd2f5209e",
   "metadata": {},
   "source": [
    "### __1. Import and data pre-processing:__\n",
    "<font size=3>\n",
    "    \n",
    "1.1 Import data;\\\n",
    "1.2 Data visualization;\\\n",
    "1.3 Feature engineering;\\\n",
    "1.4 Data shuffling;\\\n",
    "1.5 Train, validation, and test tensor divition.\n",
    "\n",
    "Our first task will classify _normal_ and _abnormal_ orthopedic diagnoses from biomechanical features. Our data can be downloaded from the [Kaggle datasets](https://www.kaggle.com/datasets/uciml/biomechanical-features-of-orthopedic-patients?resource=download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc798d16-acd6-4eb6-a9f9-2bfd01d7b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/column_2C_weka.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981599f-b0ba-4e3a-8355-24a25c173bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many classes do we have?\n",
    "\n",
    "n_normal = len(df[df['class'] == 'Normal'])\n",
    "n_abnormal = len(df[df['class'] == 'Abnormal'])\n",
    "\n",
    "n_normal, n_abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd07cbd-a2dc-4059-b660-084e0076760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shuffling:\n",
    "i = np.random.permutation(len(df))\n",
    "\n",
    "df = df.iloc[i, :]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd85fd-edf4-4ea1-94c8-1758c4791edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features data:\n",
    "df.iloc[:, 0:6].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd56b76-d277-4956-bbb3-3d0da1e4f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 0:6].to_numpy()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef5665-4a91-42fb-86bf-4c9f026e086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target data:\n",
    "df.iloc[:, -1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebd8d9-2ddc-4f55-a941-e2ab08bfac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_fn(x):\n",
    "    if x == 'Abnormal': return 1.0\n",
    "    else: return 0.0\n",
    "\n",
    "y = df.iloc[:, -1].apply(label_fn).to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04089b08-44b2-4320-94ea-2ad69ff58056",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8e840-a3db-4656-9043-8fb683873430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y:\", y.shape)\n",
    "\n",
    "# since x array has 2 dimensions, we'll expand y dimensions:\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d32961-ac98-432c-a44f-cdbf77b56617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data:\n",
    "print(f\"x before norm: max = {x.max():.2f}, min = {x.min():.2f}\")\n",
    "\n",
    "x /= abs(x).max() # normalization: x = x/max(abs(x))\n",
    "\n",
    "print(f\"x after norm: max = {x.max():.2f}, min = {x.min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ed9d2-e240-4b7a-a2ce-cac40935becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train, validation and test:\n",
    "N_samples, N_features = x.shape\n",
    "\n",
    "N_train = int(0.70*N_samples) # 80%\n",
    "\n",
    "N_val = int(0.2*N_samples) # 20%\n",
    "\n",
    "N_test = N_samples - (N_train + N_val) # 10%\n",
    "\n",
    "print(f\"N-samples = {N_samples}, N-train = {N_train}, N-val = {N_val}, N-test = {N_test}\")\n",
    "\n",
    "print(N_samples == N_train + N_val + N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1ea25-87de-45d3-9ddf-0f52089e2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_val = x[N_train:N_train+N_val]\n",
    "y_val = y[N_train:N_train+N_val]\n",
    "\n",
    "x_test = x[N_train+N_val:]\n",
    "y_test = y[N_train+N_val:]\n",
    "\n",
    "print(f\"x-train:{x_train.shape}, y-train:{y_train.shape}\")\n",
    "print(f\"x-val:{x_val.shape}, y-val:{y_val.shape}\")\n",
    "print(f\"x-test:{x_test.shape}, y-test:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ee328-7fad-4607-b0db-af745fb7e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can delete unnecessary tansors and dataframe:\n",
    "\n",
    "del df, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297d543-bb47-455f-9cc3-0e129221f760",
   "metadata": {},
   "source": [
    "### __2. Neural network modeling:__\n",
    "<font size=3>\n",
    "    \n",
    "2.1 Define initial layer's shape;\\\n",
    "2.2 Define output layer's shape and its [activation function](https://keras.io/api/layers/activations/);\\\n",
    "2.3 Define hidden layers.\n",
    "\n",
    "[Checkout Keras API](https://keras.io/guides/functional_api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e4278-f29c-4b98-8a72-27e222c778e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In = keras.Input(shape=(x_train.shape[1],))\n",
    "\n",
    "x = keras.layers.Dense(50, activation='sigmoid')(In)\n",
    "\n",
    "Out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=In, outputs=Out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626664fc-593b-4539-ac60-4903c5734b58",
   "metadata": {},
   "source": [
    "### __3. Model compilation:__\n",
    "<font size=3>\n",
    "\n",
    "3.1 Define [optimizer](https://keras.io/api/optimizers/);\\\n",
    "3.2 Define [loss function](https://keras.io/api/losses/);\\\n",
    "3.3 Define [validation metric](https://keras.io/api/metrics/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ed652-4f95-4e71-a75e-e501c3197512",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss=\"mse\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda4521-0d6c-42fe-995f-dd52c4ae9bc1",
   "metadata": {},
   "source": [
    "### __4. Train and validation__\n",
    "<font size=3>\n",
    "    \n",
    "Here, using the training data, the optimizer updates the values of the model's inner parameters (_i.e._, weights, biases, etc.) over the epochs while minimizing/maximizing the loss function. Meanwhile, the model's performance is measured for each epoch using the validation data. At this workflow stage, we model the neural network architecture to avoid [overfitting and underfitting](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/).\n",
    "\n",
    "__Underfitting__ means a poor NN fitting, _i.e._, the model does not learn well. On the other hand, __overfitting__ occurs when the model fits the training data very well but makes poor predictions with validation data.\n",
    "\n",
    "__To avoid underfitting__, we need to make the NN more robust - with more layers and neurons - to increase the NN's depth.\n",
    "\n",
    "__To avoid overfitting__, we have two basic options: __i)__ decrease the number of neurons (or/and layers) - as an analogy, we are decreasing the degree of a polynomial function (check the [figure](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/) again); __ii)__ we can apply a [dropout layer](https://keras.io/api/layers/regularization_layers/dropout/) after the layer with the largest number of neurons.\n",
    "\n",
    "What is the dropout layer? Dropout _\"closes\"_ the activation of neurons from the previous layer at random by setting them to zero! When training becomes rigid, we create a type of _\"neuroplasticity\"_ in the network to form more flexible connections. Check the [paper](https://paperswithcode.com/method/dropout)'s motivation!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58f694-2884-4ace-b13f-4c45868d3274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee698f-f1b6-4c29-981b-a2d77657b816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae4315-260b-4ffd-8b8b-a32b3eb3b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = report.history['loss']\n",
    "val_loss = report.history['val_loss']\n",
    "\n",
    "acc = report.history['acc']\n",
    "val_acc = report.history['val_acc']\n",
    "\n",
    "epochs = np.linspace(1, len(loss), len(loss))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "ax[0].plot(epochs, loss, label='train')\n",
    "ax[0].plot(epochs, val_loss, label='val')\n",
    "ax[0].set_ylabel(\"MSE\")\n",
    "\n",
    "ax[1].plot(epochs, acc, label='train')\n",
    "ax[1].plot(epochs, val_acc, label='val')\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].grid()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8686e6-36cf-414e-8fed-238a7c2024ad",
   "metadata": {},
   "source": [
    "### __5. Final training__\n",
    "<font size=3>\n",
    "\n",
    "Once the modeling is completed, we concatenate train and validation data to fit again the model.\n",
    "\n",
    "__Note:__ use the same number of epochs from the previous step.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65e3f5-42a8-48b4-9e8f-715563883cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec8d8e-f7db-442c-8acf-49e97b515424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e875dc-7d57-42fb-a03b-19948e2de183",
   "metadata": {},
   "source": [
    "### __6. Test evaluation__:\n",
    "\n",
    "    6.1 Make the evaluation using the test data;\n",
    "    6.1 Make some predictions to visualize the results;\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef870a9-b953-4518-b8cb-ec8514151965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaf7f4-8e0b-4402-8338-d89ae52344c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1006c9-50c7-4341-a265-a9221ae1ebca",
   "metadata": {},
   "source": [
    "### __7. Saving the model__:\n",
    "<font size=3>\n",
    "    \n",
    "[Checkout](https://keras.io/api/models/model_saving_apis/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
