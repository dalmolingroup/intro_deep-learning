{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519d2897-55a9-45e2-a04c-d13933eb1b7c",
   "metadata": {},
   "source": [
    "## __K-fold cross-validataion__\n",
    "<font size=3>\n",
    "\n",
    "K-fold cross-validation is an effective technique for training NN when dealing with __limited datasets__. Small datasets ($< \\; \\sim 10^3$) often result in _underfitting_ and poor statistical model evaluations, leading to inconsistent performance. For instance, a model may perform well once during training but worsen if we do it again. This variation occurs because small datasets often lack feature homogeneity, meaning the subset used for training might not always represent the underlying patterns in the data.\n",
    "\n",
    "To address this, K-fold cross-validation divides the training data into $K$ equal parts (see figure below). In each iteration\n",
    "* one part is reserved for validation, while the remaining parts are used for training.\n",
    "* After each training cycle, the model's performance is recorded, the parameters are reset,\n",
    "* and the process is repeated $K-1$ more times, each time using a different part for validation.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/kfold.png\" width=\"450\"/>\n",
    "</center>\n",
    "\n",
    "At the end of this process, the model’s performance metrics are averaged across all K-folds, providing a mean performance score along with its standard deviation. This approach offers a more robust estimate of the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87343a-db66-43b9-8bc0-c822f5e8c959",
   "metadata": {},
   "source": [
    "### __How to do in practice?__\n",
    "<font size=3>\n",
    "    \n",
    "Considering we already separated the dataset into $(\\mathtt x,\\, \\mathtt{x\\_test})$, where $\\mathtt x$ we take for k-fold cross-validation.\\\n",
    "If $\\mathtt x$ size is 300, and we choose $\\mathtt K=3$, the $\\mathtt{N\\_val} = 100$. Check the code below.\n",
    "\n",
    "<font size=2>\n",
    "    \n",
    "```python\n",
    "\n",
    "K = 3\n",
    "N_val = len(x)//K\n",
    "k_hist = []\n",
    "\n",
    "for k in range(K):\n",
    "\n",
    "    # 1. splitting the data into train and validation:\n",
    "    x_val = x[k*N_val:(k+1)*N_val]\n",
    "    x_train = np.concatenate([x[:k*N_val], x[(k+1)*N_val:]])\n",
    "\n",
    "    # 2. reset model:\n",
    "    model = MyModel()\n",
    "    \n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # 3. fittting the model:\n",
    "    report = model.fit(x_train, validation_data=[x_val], epochs=50, batch_size=1)\n",
    "\n",
    "    # 4. recording performance:\n",
    "    k_hist.append(report.history)\n",
    "\n",
    "```\n",
    "<br>\n",
    "<font size=3>\n",
    "\n",
    "For $\\mathtt k = 0$, we have $\\mathtt{x\\_val = x[0:100]}$ and $\\mathtt{x\\_train = cat[x[:0], x[100:]]}$.\n",
    "\n",
    "For $\\mathtt k = 1$, we have $\\mathtt{x\\_val = x[100:200]}$ and $\\mathtt{x\\_train = cat[x[:100], x[200:]]}$.\n",
    "\n",
    "For $\\mathtt k = 2$, we have $\\mathtt{x\\_val = x[200:300]}$ and $\\mathtt{x\\_train = cat[x[:200], x[300:]]}$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
