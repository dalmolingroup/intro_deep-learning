{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1883a14-719b-4c95-925c-cc2152a80e3d",
   "metadata": {},
   "source": [
    "## __Natural Language Processing__\n",
    "<font size=3>\n",
    "\n",
    "Deep learning for sequence processing has made significant advances with the introduction of dense and convolutional layers. However, working with time-series data presents unique challenges, especially in forecasting tasks and maintaining sequential memory to create meaningful long feature chains.\n",
    "\n",
    "One of the most complex forms of sequential data is [natural language](https://en.wikipedia.org/wiki/Natural_language#:~:text=In%20neuropsychology%2C%20linguistics%2C%20and%20philosophy,without%20conscious%20planning%20or%20premeditation.). Natural language includes any form of human expression, whether in audio, images, or, most commonly for data, texts. Deep learning tasks involving natural language, such as classification, sentiment analysis, question-answering, and translation, require special models to understand and interpret these intricate patterns. In this section, we will explore how deep learning models \"read\" texts and perform [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb207ab5-8d52-4683-93a0-304f14ed38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 13:15:53.171226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 13:15:54.184921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab136a1-c879-4522-b4d4-c29581f59f0f",
   "metadata": {},
   "source": [
    "### __Text Vectorization:__\n",
    "<font size=3>\n",
    "\n",
    "Since a neural network (NN) model is a non-linear function that computes weights and biases to process data, how are texts handled? Text processing is achieved by mapping strings or characters into numerical vectors - a process known as __vectorization__.\n",
    "\n",
    "To illustrate, let's use the [Zen of Python](https://peps.python.org/pep-0020/) statements as our dataset/__corpus__ of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c57990b-6023-405c-aaed-d82e0512ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Beautiful is better than ugly\",\n",
    "          \"Explicit is better than implicit\",\n",
    "          \"Simple is better than complex\",\n",
    "          \"Complex is better than complicated\",\n",
    "          \"Flat is better than nested\",\n",
    "          \"Sparse is better than dense\",\n",
    "          \"Readability counts\",\n",
    "          \"Special cases aren't special enough to break the rules\",\n",
    "          \"Although practicality beats purity\",\n",
    "          \"Errors should never pass silently\",\n",
    "          \"Unless explicitly silenced\",\n",
    "          \"In the face of ambiguity, refuse the temptation to guess\",\n",
    "          \"There should be one -and preferably only one- obvious way to do it\",\n",
    "          \"Although that way may not be obvious at first unless you're Dutch\",\n",
    "          \"Now is better than never\",\n",
    "          \"Although never is often better than right now\",\n",
    "          \"If the implementation is hard to explain, it's a bad idea\",\n",
    "          \"If the implementation is easy to explain, it may be a good idea\",\n",
    "          \"Namespaces are one honking great idea -let's do more of those!\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90784bb8-fcad-4ef0-b870-72fbdecbd1c8",
   "metadata": {},
   "source": [
    "### __1. Tokenization:__\n",
    "<font size=3>\n",
    "\n",
    "To vectorize a sentence, we must first define a strategy for separating characters or words to correlate them to numbers. These pieces of the sentence are commonly referred to as tokens. There are three main forms of tokenization, so, let's consider the first statement for illustration:\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "   ```python\n",
    "    sentence = [\"Beautiful is better than ugly\"]\n",
    "             \n",
    "   ```\n",
    "<font size=3>\n",
    "    \n",
    "- __Character to vector__:\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "   ```python\n",
    "    tokens = ['B', 'e', 'a', 'u', 't', 'i', 'f', 'u', 'l', 'i', 's', \n",
    "              'b', 'e', 't', 't', 'e', 'r', 't', 'h', 'a', 'n', \n",
    "              'u', 'g', 'l', 'y']\n",
    "             \n",
    "   ```\n",
    "<font size=3>\n",
    "\n",
    "- __Word to vector__:\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "   ```python\n",
    "    tokens = ['Beautiful', 'is', 'better', 'than', 'ugly']\n",
    "             \n",
    "   ```\n",
    "<font size=3>\n",
    "\n",
    "- __N-gram to vector__:\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "   ```python\n",
    "    # 1-grams/unigram\n",
    "    tokens = ['Beautiful', 'is', 'better', 'than', 'ugly']\n",
    "\n",
    "    # 2-grams/bigram\n",
    "    tokens = ['Beautiful', 'Beautiful is', 'is', 'is better', \n",
    "              'better', 'better than', 'than', 'than ugly', 'ugly'] \n",
    "    \n",
    "   ```\n",
    "<font size=3>\n",
    "\n",
    "   These forms of [n-gram](https://en.wikipedia.org/wiki/N-gram) are called _bag-of-1-gram_ and _bag-of-2-grams_.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The chosen strategy will depend on the task, of course, but we will now work with _word to vector_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7c0ad9-1c85-4093-940e-022c61189fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Beautiful', 'is', 'better', 'than', 'ugly'],\n",
       " ['Explicit', 'is', 'better', 'than', 'implicit'],\n",
       " ['Simple', 'is', 'better', 'than', 'complex'],\n",
       " ['Complex', 'is', 'better', 'than', 'complicated']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [text.split() for text in corpus]\n",
    "token_list[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53842323-9c65-4fc3-89d5-0cfe1689d1fe",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "After tokenization, sentence vectorization can be performed using two main methods: __one-hot encoding__ and __token/word-embeddings__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11f393-537c-4f86-bf94-e01eaeb94426",
   "metadata": {},
   "source": [
    "### __2. One-hot encoding__:\n",
    "<font size=3>\n",
    "\n",
    "To do so, we will\n",
    "- define the vocabulary dictionary;\n",
    "- correlate a word with an integer index;\n",
    "- transform the sentences into index list;\n",
    "- one-hot encoding the index list.\n",
    "  \n",
    "by handmade and by Keras API. \n",
    "\n",
    "#### __2.1 Handmade:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb974784-d700-4865-8a7b-0ab60b847583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 1,\n",
       " 'is': 2,\n",
       " 'better': 3,\n",
       " 'than': 4,\n",
       " 'ugly': 5,\n",
       " 'explicit': 6,\n",
       " 'implicit': 7,\n",
       " 'simple': 8,\n",
       " 'complex': 9,\n",
       " 'complicated': 10,\n",
       " 'flat': 11,\n",
       " 'nested': 12,\n",
       " 'sparse': 13,\n",
       " 'dense': 14,\n",
       " 'readability': 15,\n",
       " 'counts': 16,\n",
       " 'special': 17,\n",
       " 'cases': 18,\n",
       " 'arent': 19,\n",
       " 'enough': 20,\n",
       " 'to': 21,\n",
       " 'break': 22,\n",
       " 'the': 23,\n",
       " 'rules': 24,\n",
       " 'although': 25,\n",
       " 'practicality': 26,\n",
       " 'beats': 27,\n",
       " 'purity': 28,\n",
       " 'errors': 29,\n",
       " 'should': 30,\n",
       " 'never': 31,\n",
       " 'pass': 32,\n",
       " 'silently': 33,\n",
       " 'unless': 34,\n",
       " 'explicitly': 35,\n",
       " 'silenced': 36,\n",
       " 'in': 37,\n",
       " 'face': 38,\n",
       " 'of': 39,\n",
       " 'ambiguity': 40,\n",
       " 'refuse': 41,\n",
       " 'temptation': 42,\n",
       " 'guess': 43,\n",
       " 'there': 44,\n",
       " 'be': 45,\n",
       " 'one': 46,\n",
       " 'and': 47,\n",
       " 'preferably': 48,\n",
       " 'only': 49,\n",
       " 'obvious': 50,\n",
       " 'way': 51,\n",
       " 'do': 52,\n",
       " 'it': 53,\n",
       " 'that': 54,\n",
       " 'may': 55,\n",
       " 'not': 56,\n",
       " 'at': 57,\n",
       " 'first': 58,\n",
       " 'youre': 59,\n",
       " 'dutch': 60,\n",
       " 'now': 61,\n",
       " 'often': 62,\n",
       " 'right': 63,\n",
       " 'if': 64,\n",
       " 'implementation': 65,\n",
       " 'hard': 66,\n",
       " 'explain': 67,\n",
       " 'its': 68,\n",
       " 'a': 69,\n",
       " 'bad': 70,\n",
       " 'idea': 71,\n",
       " 'easy': 72,\n",
       " 'good': 73,\n",
       " 'namespaces': 74,\n",
       " 'are': 75,\n",
       " 'honking': 76,\n",
       " 'great': 77,\n",
       " 'lets': 78,\n",
       " 'more': 79,\n",
       " 'those': 80}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the vocabulary dictionary:\n",
    "\n",
    "def reform(word):\n",
    "    '''\n",
    "    - Lowercase the words;\n",
    "    - Removing punctuations.\n",
    "    '''\n",
    "    word = word.lower()\n",
    "    return word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "vocab_dict = {}\n",
    "for text in corpus:\n",
    "    for word in text.split():\n",
    "        \n",
    "        word = reform(word)\n",
    "        \n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word] = len(vocab_dict) + 1\n",
    "\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3957e-e417-4769-b10c-5dd94bd61f1e",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "The $\\mathtt{vocab\\_dict}$ associates each token/word with an index. To do that, we lowercase the word to avoid something like\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "```python\n",
    "    vocab_dict = {\"The\":1, \"the\":2, ...} ,\n",
    "    \n",
    "```\n",
    "<font size=3>\n",
    "\n",
    "and we remove the punctuation to avoid\n",
    "\n",
    "<font size=2.5>\n",
    "\n",
    "```python\n",
    "    vocab_dict = {..., \"better\":23, \"better!\":24, ...} .\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb86f56-c892-4118-8207-18366934ebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punctuations to be removed:\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231e997-15cd-4a20-8e87-0d9d1325909e",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Since sentences can vary in length and NN architectures require fixed input sizes, we need to define a maximum length, $\\mathtt{max\\_len}$, for all sentences. Sentences shorter than $\\mathtt{max\\_len}$ are padded with zeros, while those longer than $\\mathtt{max\\_len}$ are truncated. Each word in the text is assigned a unique index (different from zero), with the index 0 reserved for the padding. So, let's include the \"[PAD]\" flag into the $\\mathtt{vocab\\_dict}$ and reform $\\mathtt{corpus}$' sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de44dd2-c1b7-4dd9-85cf-5c81f500df39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beautiful is better than ugly [PAD] [PAD]',\n",
       " 'Explicit is better than implicit [PAD] [PAD]',\n",
       " 'Simple is better than complex [PAD] [PAD]',\n",
       " 'Complex is better than complicated [PAD] [PAD]',\n",
       " 'Flat is better than nested [PAD] [PAD]',\n",
       " 'Sparse is better than dense [PAD] [PAD]',\n",
       " 'Readability counts [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " \"Special cases aren't special enough to break\",\n",
       " 'Although practicality beats purity [PAD] [PAD] [PAD]',\n",
       " 'Errors should never pass silently [PAD] [PAD]',\n",
       " 'Unless explicitly silenced [PAD] [PAD] [PAD] [PAD]',\n",
       " 'In the face of ambiguity, refuse the',\n",
       " 'There should be one -and preferably only',\n",
       " 'Although that way may not be obvious',\n",
       " 'Now is better than never [PAD] [PAD]',\n",
       " 'Although never is often better than right',\n",
       " 'If the implementation is hard to explain,',\n",
       " 'If the implementation is easy to explain,',\n",
       " \"Namespaces are one honking great idea -let's\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 7\n",
    "vocab_dict[\"[PAD]\"] = 0\n",
    "\n",
    "corpus_pad = [ ]\n",
    "for text in corpus:\n",
    "    token_list = text.split()\n",
    "    \n",
    "    if len(token_list) < max_len:\n",
    "        text += (max_len-len(token_list))*\" [PAD]\"\n",
    "        \n",
    "    else:\n",
    "        text = \" \".join(token_list[:max_len])\n",
    "\n",
    "    corpus_pad.append(text)\n",
    "\n",
    "corpus_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5effa1-02ba-4162-8048-48aea6224631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 7, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(vocab_dict.values()) # maximum token index\n",
    "\n",
    "onehot = np.zeros((len(corpus), max_len, vocab_size + 1)) # \"+1\" includes the padding index\n",
    "\n",
    "for i, text in enumerate(corpus_pad):\n",
    "    for j, word in enumerate(text.split()):\n",
    "        if word != \"[PAD]\": \n",
    "            word = reform(word)\n",
    "\n",
    "        index = vocab_dict.get(word)        \n",
    "        onehot[i, j, index] = 1\n",
    "\n",
    "onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e4c0a9-d599-4b26-9a3a-c807f89c868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "is [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "better [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "than [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "ugly [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "[PAD] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "[PAD] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "i = 0\n",
    "text = corpus_pad[i].split()\n",
    "\n",
    "for word, vec in zip(text, onehot[i]):\n",
    "    if word != \"[PAD]\": \n",
    "        word = reform(word)\n",
    "    \n",
    "    print(word, vec, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae8507-5d6a-41e6-8659-adad53895208",
   "metadata": {},
   "source": [
    "#### __2.2 Using Keras:__\n",
    "<font size=3>\n",
    "\n",
    "To vectorize sentence using Keras API, we will use the [TextVectorization layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f80d33f-1034-4528-8bfa-c554f054b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 80\n",
      "Vocabulary tokens: ['', '[UNK]', 'is', 'than', 'better', 'to', 'the', 'one', 'never', 'idea', 'be', 'although', 'way', 'unless', 'special', 'should', 'of', 'obvious', 'now', 'may', 'it', 'implementation', 'if', 'explain', 'do', 'complex', 'a', 'youre', 'ugly', 'those', 'there', 'that', 'temptation', 'sparse', 'simple', 'silently', 'silenced', 'rules', 'right', 'refuse', 'readability', 'purity', 'preferably', 'practicality', 'pass', 'only', 'often', 'not', 'nested', 'namespaces', 'more', 'lets', 'its', 'in', 'implicit', 'honking', 'hard', 'guess', 'great', 'good', 'flat', 'first', 'face', 'explicitly', 'explicit', 'errors', 'enough', 'easy', 'dutch', 'dense', 'counts', 'complicated', 'cases', 'break', 'beautiful', 'beats', 'bad', 'at', 'arent', 'are']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 13:15:56.214522: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "max_len = 7\n",
    "\n",
    "vectorize = layers.TextVectorization(max_tokens=vocab_size,\n",
    "                                     standardize='lower_and_strip_punctuation',\n",
    "                                     split='whitespace',\n",
    "                                     output_mode='int',\n",
    "                                     output_sequence_length=max_len)\n",
    "\n",
    "# get vocabulary from corpus:\n",
    "vectorize.adapt(corpus)\n",
    "\n",
    "# get token indexes from corpus:\n",
    "token_ids = vectorize(corpus)\n",
    "\n",
    "# [UNK] = unknown word\n",
    "print(\"Vocabulary size:\", vectorize.vocabulary_size())\n",
    "print(\"Vocabulary tokens:\", vectorize.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdaeb733-f0e5-4a8f-b4d2-b395f7d15387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 7), dtype=int64, numpy=\n",
       "array([[74,  2,  4,  3, 28,  0,  0],\n",
       "       [64,  2,  4,  3, 54,  0,  0],\n",
       "       [34,  2,  4,  3, 25,  0,  0],\n",
       "       [25,  2,  4,  3, 71,  0,  0],\n",
       "       [60,  2,  4,  3, 48,  0,  0],\n",
       "       [33,  2,  4,  3, 69,  0,  0],\n",
       "       [40, 70,  0,  0,  0,  0,  0],\n",
       "       [14, 72, 78, 14, 66,  5, 73],\n",
       "       [11, 43, 75, 41,  0,  0,  0],\n",
       "       [65, 15,  8, 44, 35,  0,  0],\n",
       "       [13, 63, 36,  0,  0,  0,  0],\n",
       "       [53,  6, 62, 16,  1, 39,  6],\n",
       "       [30, 15, 10,  7,  1, 42, 45],\n",
       "       [11, 31, 12, 19, 47, 10, 17],\n",
       "       [18,  2,  4,  3,  8,  0,  0],\n",
       "       [11,  8,  2, 46,  4,  3, 38],\n",
       "       [22,  6, 21,  2, 56,  5, 23],\n",
       "       [22,  6, 21,  2, 67,  5, 23],\n",
       "       [49, 79,  7, 55, 58,  9, 51]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0770a505-1b7b-4f5e-be17-5a56911bac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.zeros((len(corpus), max_len, vocab_size + 1))\n",
    "\n",
    "for i, ids in enumerate(token_ids):\n",
    "    for j, ID in enumerate(ids):\n",
    "        onehot[i][j][ID] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf31b60-4504-4c84-94cc-325a8f42313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "is [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "better [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "than [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "ugly [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "[PAD] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "[PAD] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "i = 0\n",
    "text = corpus[i].split()\n",
    "\n",
    "if len(text) < max_len:\n",
    "    text += [\"[PAD]\"]*(max_len-len(text))\n",
    "\n",
    "for word, vec in zip(text, onehot[0]):\n",
    "    if word != \"[PAD]\": \n",
    "        word = reform(word)\n",
    "    \n",
    "    print(word, vec, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869aa82-ef4e-4ea2-8b48-7f2ebd4b4ced",
   "metadata": {},
   "source": [
    "#### __2.3 In summary:__ one-hot encoding vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de15fc54-2c5e-4325-954e-0111471c647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simple is better than complex'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the sentence:\n",
    "sentence = corpus[2]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00902ab-d9c0-414b-aa35-ca39ae29ced7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simple', 'is', 'better', 'than', 'complex', '[PAD]', '[PAD]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. tokenization and padding:\n",
    "token_list = sentence.split()\n",
    "\n",
    "if len(token_list) < max_len:\n",
    "    token_list += (max_len-len(token_list))*[\"[PAD]\"]\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b2649c-1045-4ee6-9558-3478f0b201b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 3, 4, 9, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. get token IDs:\n",
    "token_ids = []\n",
    "for word in token_list:\n",
    "    \n",
    "    if word != \"[PAD]\":\n",
    "        word = reform(word)\n",
    "\n",
    "    token_ids.append(vocab_dict.get(word))\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34dbcd20-565d-4cfd-8106-d4c391530e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. one-hot encoding:\n",
    "max_len = 7\n",
    "onehot = np.zeros((max_len, vocab_size+1))\n",
    "\n",
    "for i, ID in enumerate(token_ids):\n",
    "    onehot[i][ID] = 1\n",
    "\n",
    "'''\n",
    "Now, the onehot tensor is ready to feed the NN model!\n",
    "'''\n",
    "\n",
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880c7c4-941f-4aa5-a34d-5471d00d9325",
   "metadata": {},
   "source": [
    "### __3. Word-embedding:__\n",
    "<font size=3>\n",
    "\n",
    "It's important to note that using one-hot encoding to represent an entire corpus vocabulary results in large and sparse vectors, which can be inefficient in terms of memory and computation. As an alternative, we will use the __word embedding__ method to vectorize the corpus. This approach produces dense and compact representations, making it more efficient and meaningful. We will explore word embeddings in detail in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a1a19-e319-4e0a-b90c-3eeac049ad4d",
   "metadata": {},
   "source": [
    "### __References:__\n",
    "<font size=3>\n",
    "    \n",
    "- [Deep Learning with Python](https://books.google.com.br/books/about/Deep_Learning_with_Python.html?id=Yo3CAQAACAAJ&redir_esc=y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
