{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9ebf481-5eb7-443e-a5c2-45d8e1c0051f",
   "metadata": {},
   "source": [
    "### __3. Word-Embedding:__\n",
    "<font size=3>\n",
    "\n",
    "Word-embeddings represent words or tokens as dense vectors composed of float numbers, allowing for significantly lower-dimensional representations compared to one-hot encoding. These vectors are either learned during training or derived from pre-trained embeddings, which are especially useful for smaller datasets. \n",
    "\n",
    "For example, a word represented in one-hot encoding as [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] could have an embedding form like [0.245 -0.183 0.834]. The length of the one-hot vector corresponds to the vocabulary size ($\\mathtt{vocab\\_size}$), while the length of the embedding vector is determined by the embedding-dimension ($\\mathtt{embed\\_dim}$), a predefined hyperparameter.\n",
    "\n",
    "#### __3.1 How it works in practice:__\n",
    "    \n",
    "- We import the corpus;\n",
    "- Transform each sentence into a list of token IDs;\n",
    "- And make the word-embedding.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9575eeb8-7472-4873-bc6f-2b13af02724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 11:56:39.799568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 11:56:40.823977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21239d5d-8626-4f50-868e-60e948840a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import corpus:\n",
    "corpus = [\"Beautiful is better than ugly\",\n",
    "          \"Explicit is better than implicit\",\n",
    "          \"Simple is better than complex\",\n",
    "          \"Complex is better than complicated\",\n",
    "          \"Flat is better than nested\",\n",
    "          \"Sparse is better than dense\",\n",
    "          \"Readability counts\",\n",
    "          \"Special cases aren't special enough to break the rules\",\n",
    "          \"Although practicality beats purity\",\n",
    "          \"Errors should never pass silently\",\n",
    "          \"Unless explicitly silenced\",\n",
    "          \"In the face of ambiguity, refuse the temptation to guess\",\n",
    "          \"There should be one -- and preferably only one -- obvious way to do it\",\n",
    "          \"Although that way may not be obvious at first unless you're Dutch\",\n",
    "          \"Now is better than never\",\n",
    "          \"Although never is often better than right now\",\n",
    "          \"If the implementation is hard to explain, it's a bad idea\",\n",
    "          \"If the implementation is easy to explain, it may be a good idea\",\n",
    "          \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e86114c-0e3d-411a-8cab-15a64fa0bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82\n",
      "Vocabulary tokens: ['', '[UNK]', 'is', 'than', 'better', 'to', 'the', 'one', 'never', 'idea', 'be', 'although', 'way', 'unless', 'special', 'should', 'of', 'obvious', 'now', 'may', 'it', 'implementation', 'if', 'explain', 'do', 'complex', 'a', 'youre', 'ugly', 'those', 'there', 'that', 'temptation', 'sparse', 'simple', 'silently', 'silenced', 'rules', 'right', 'refuse', 'readability', 'purity', 'preferably', 'practicality', 'pass', 'only', 'often', 'not', 'nested', 'namespaces', 'more', 'lets', 'its', 'in', 'implicit', 'honking', 'hard', 'guess', 'great', 'good', 'flat', 'first', 'face', 'explicitly', 'explicit', 'errors', 'enough', 'easy', 'dutch', 'dense', 'counts', 'complicated', 'cases', 'break', 'beautiful', 'beats', 'bad', 'at', 'arent', 'are', 'and', 'ambiguity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 11:56:42.792246: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# 2. transform each sentence into a list of token IDs:\n",
    "\n",
    "vocab_size = None # maximum vocabulary size\n",
    "max_len = 7 # maximum sentence length\n",
    "\n",
    "vectorize = layers.TextVectorization(max_tokens=vocab_size,\n",
    "                                    standardize='lower_and_strip_punctuation',\n",
    "                                    split='whitespace',\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_len)\n",
    "\n",
    "vectorize.adapt(corpus)\n",
    "\n",
    "vocab = vectorize.get_vocabulary()\n",
    "vocab_size = vectorize.vocabulary_size()\n",
    "\n",
    "# get list of token IDs:\n",
    "token_ids = vectorize(corpus)\n",
    "\n",
    "# [UNK] = unknown word\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Vocabulary tokens:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8713ab-52f7-4f89-a782-e04b0c62a7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01171773, -0.04461163,  0.04308352],\n",
       "       [ 0.00517938, -0.04085418, -0.00716002],\n",
       "       [ 0.03742431, -0.01960806, -0.03429973],\n",
       "       [ 0.0404867 ,  0.00247454, -0.03970286],\n",
       "       [ 0.00722747, -0.03345873,  0.00917314],\n",
       "       [-0.03696976,  0.04288835,  0.02702333],\n",
       "       [-0.00140247,  0.04975105,  0.02677205],\n",
       "       [-0.03489292,  0.04262637, -0.00257399],\n",
       "       [-0.01237092, -0.0342587 , -0.01603409],\n",
       "       [ 0.04773648, -0.03987242, -0.04603728],\n",
       "       [ 0.04338927, -0.01980357,  0.04309079],\n",
       "       [-0.00552287, -0.00997583,  0.03867776],\n",
       "       [ 0.01270385, -0.00379841, -0.02461885],\n",
       "       [ 0.01459284,  0.04433655, -0.03203994],\n",
       "       [ 0.04589163, -0.03375693, -0.04510615],\n",
       "       [ 0.01854787, -0.04475548, -0.0300722 ],\n",
       "       [ 0.02379323,  0.03466045, -0.00621045],\n",
       "       [ 0.00888406, -0.01606665, -0.03407723],\n",
       "       [-0.03982375,  0.0464437 ,  0.00847567],\n",
       "       [-0.0282846 , -0.04127765,  0.02778846],\n",
       "       [-0.04867227, -0.02617785, -0.04002528],\n",
       "       [-0.03512352,  0.03853056, -0.01293579],\n",
       "       [ 0.03401542,  0.04973835,  0.04105283],\n",
       "       [ 0.01827171,  0.03874296,  0.00433936],\n",
       "       [ 0.02834812,  0.04574025,  0.03091338],\n",
       "       [ 0.00933928, -0.00057039, -0.03497469],\n",
       "       [-0.02060934, -0.02646815, -0.00555874],\n",
       "       [-0.01874612,  0.0435137 , -0.03668343],\n",
       "       [ 0.029376  , -0.02076438,  0.03059622],\n",
       "       [-0.00811118, -0.02545737, -0.03335974],\n",
       "       [ 0.01969788,  0.01062559, -0.01480204],\n",
       "       [-0.00700172,  0.01983065,  0.03480012],\n",
       "       [ 0.02180814, -0.02441398, -0.01804029],\n",
       "       [ 0.00560369, -0.01136978,  0.00949324],\n",
       "       [-0.04684195, -0.02802321, -0.03124937],\n",
       "       [ 0.01029795,  0.0126814 ,  0.02845151],\n",
       "       [-0.00870246, -0.00093886,  0.03681569],\n",
       "       [-0.04108389,  0.02540164,  0.00323264],\n",
       "       [ 0.01600257, -0.02965045, -0.03765936],\n",
       "       [ 0.03651433,  0.04603169, -0.0386015 ],\n",
       "       [ 0.02311689, -0.01708498, -0.04014096],\n",
       "       [ 0.04431934,  0.0172248 , -0.02272686],\n",
       "       [-0.01209105,  0.04108617,  0.04191426],\n",
       "       [ 0.04042623, -0.02163451,  0.00696737],\n",
       "       [ 0.00636324,  0.00751861, -0.04937796],\n",
       "       [-0.03552096,  0.03892979, -0.02720318],\n",
       "       [ 0.04038623, -0.0372918 ,  0.01252815],\n",
       "       [-0.01661449, -0.01185465, -0.03324571],\n",
       "       [ 0.01775327, -0.03482486, -0.0094097 ],\n",
       "       [ 0.02295149, -0.02628434,  0.00741527],\n",
       "       [ 0.01019222, -0.00789047,  0.04610957],\n",
       "       [-0.00435001, -0.04680313,  0.00534938],\n",
       "       [-0.00980343,  0.03713629, -0.02932403],\n",
       "       [ 0.04350782, -0.0349852 , -0.01645446],\n",
       "       [-0.02554255,  0.04616823,  0.03342033],\n",
       "       [-0.02323506, -0.03365307,  0.01744893],\n",
       "       [ 0.01810253, -0.0003906 , -0.03790225],\n",
       "       [-0.01484673, -0.00363914,  0.00165135],\n",
       "       [ 0.02424487,  0.02679488,  0.02654885],\n",
       "       [-0.03899118, -0.00850474,  0.03229048],\n",
       "       [-0.030943  ,  0.04432763, -0.04211968],\n",
       "       [ 0.01717082, -0.04675459,  0.03714564],\n",
       "       [-0.02122796, -0.03835236,  0.00514841],\n",
       "       [ 0.04462203, -0.0241241 ,  0.03537582],\n",
       "       [-0.02215339,  0.03958808, -0.0365255 ],\n",
       "       [ 0.02031657,  0.015388  ,  0.01225803],\n",
       "       [ 0.03715951,  0.00532017,  0.01003213],\n",
       "       [-0.01495429, -0.01635004,  0.00438924],\n",
       "       [-0.04307219,  0.00289977,  0.00759469],\n",
       "       [ 0.02488748,  0.02013211,  0.04288832],\n",
       "       [-0.02458203,  0.00639059,  0.00576601],\n",
       "       [ 0.03885721, -0.02224015,  0.01553906],\n",
       "       [ 0.01055657,  0.00492435, -0.04014675],\n",
       "       [-0.04112632, -0.02881272, -0.04818676],\n",
       "       [ 0.00556556,  0.04350587,  0.03800544],\n",
       "       [-0.03126872, -0.04551641, -0.01841725],\n",
       "       [-0.00099096,  0.03159231, -0.03077468],\n",
       "       [-0.01823262, -0.0487123 ,  0.03472498],\n",
       "       [ 0.03370876,  0.04636438, -0.04615092],\n",
       "       [ 0.0183454 , -0.04755602,  0.02036387],\n",
       "       [-0.00717175, -0.04265164, -0.03181776],\n",
       "       [ 0.04403278, -0.02985704,  0.03608699]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. make word-embedding:\n",
    "\n",
    "embed_dim = 3\n",
    "embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "# define input shape to initialize embedding weights:\n",
    "embedding.build(input_shape=(token_ids.shape))\n",
    "\n",
    "# print embedding weights - an array of shape (vocab_size, embed_dim):\n",
    "embedding.weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1d0c36-f413-45a2-8f6c-07fe4df9cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedded tokens\n",
    "embed_tokens = embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91b029c-c91d-451f-b75f-92042195044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence:\n",
      "Beautiful is better than ugly\n",
      "\n",
      "- Token IDs:\n",
      "[74  2  4  3 28  0  0]\n",
      "\n",
      "- Embedded tokes:\n",
      "[[ 0.00556556  0.04350587  0.03800544]\n",
      " [ 0.03742431 -0.01960806 -0.03429973]\n",
      " [ 0.00722747 -0.03345873  0.00917314]\n",
      " [ 0.0404867   0.00247454 -0.03970286]\n",
      " [ 0.029376   -0.02076438  0.03059622]\n",
      " [-0.01171773 -0.04461163  0.04308352]\n",
      " [-0.01171773 -0.04461163  0.04308352]]\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "\n",
    "i = 0\n",
    "print(f\"- Sentence:\\n{corpus[i]}\\n\")\n",
    "print(f\"- Token IDs:\\n{token_ids[i]}\\n\")\n",
    "print(f\"- Embedded tokes:\\n{embed_tokens[0].numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5f7e47c-007d-4258-927a-e1cf85886ddf",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Note above that the token IDs __are__ the embedding weights row index! So each word is mapped by a vector of size $\\mathtt{embed\\_dim}$.\n",
    "\n",
    "#### __3.2 Pretrained word-embedding:__\n",
    "\n",
    "When working with a small dataset or aiming to reduce training computational costs, pretrained word embeddings can be highly beneficial. Popular [pretrained word-embedding](https://keras.io/examples/nlp/pretrained_word_embeddings/) include [Word2vec](https://code.google.com/archive/p/word2vec) and [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove). In this example, we will vectorize our _Zen of Python_ corpus using GloVe embeddings. \n",
    "\n",
    "To get started, we need to download the GloVe dataset using the cell below. The downloaded zip file contains embeddings with four different dimensional representations (50D, 100D, 200D, and 300D). For this task, we will focus on $\\mathtt{embed\\_dim=100}$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fed18cdc-3992-4aac-ac4d-a8d796722e02",
   "metadata": {},
   "source": [
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6960a321-696b-4402-8a15-261958886a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-7.9761e-02,  1.9551e-01,  3.0579e-01, -2.1571e-01, -4.9017e-01,\n",
       "        4.6350e-01, -1.5171e-01, -1.6002e-01,  1.3081e-01, -6.5718e-01,\n",
       "       -1.1343e-01,  1.0231e-01,  1.1583e-01,  2.0241e-03,  1.8107e-01,\n",
       "       -1.8263e-01, -4.2386e-01,  5.6726e-02, -3.0419e-01,  1.5828e-01,\n",
       "       -1.1820e-01,  1.8624e-01, -5.2731e-01, -5.9154e-01,  7.1546e-02,\n",
       "        1.9633e-01, -4.9147e-02, -3.3004e-01,  5.0489e-01,  5.1138e-01,\n",
       "       -5.0726e-01,  7.9255e-01,  1.7890e-01,  3.5001e-01, -7.2015e-02,\n",
       "        8.9293e-01, -2.7286e-01, -5.7761e-01,  1.8615e-01, -9.8489e-02,\n",
       "       -6.1398e-01,  6.1104e-02, -3.3847e-01, -2.9190e-01, -7.1794e-01,\n",
       "       -3.7329e-01, -3.2193e-01, -3.8184e-01,  4.9009e-02, -1.2856e+00,\n",
       "        3.1266e-02,  1.2953e-01,  1.1391e-01,  6.9458e-01,  3.3839e-01,\n",
       "       -2.1965e+00,  8.4632e-02,  7.6947e-02,  9.7508e-01,  3.2743e-01,\n",
       "        2.8664e-01,  7.9778e-01, -4.9729e-01, -1.1200e+00,  9.1580e-01,\n",
       "        8.9064e-02,  1.1378e+00,  3.3187e-01, -1.8245e-01,  1.7541e-01,\n",
       "        9.8961e-02, -3.9566e-01, -4.1590e-01, -7.4777e-01, -4.6913e-01,\n",
       "        3.8674e-01,  2.7161e-01, -1.5303e-02, -5.2653e-01, -2.0984e-01,\n",
       "        1.2046e-01, -4.0667e-01,  2.9756e-01, -1.3695e-01, -1.3846e+00,\n",
       "       -1.4904e-01, -4.8938e-01,  7.5170e-01, -2.2143e-01, -5.2081e-01,\n",
       "        2.6477e-01,  2.7790e-01, -4.1847e-01, -2.1104e-01, -7.4714e-01,\n",
       "       -6.5609e-02, -2.6370e-01,  2.0017e-01,  8.7429e-01,  6.9208e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get token vectors:\n",
    "embed_dict = {}\n",
    "\n",
    "with open(\"../dataset/glove.6B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embed_dict[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embed_dict))\n",
    "\n",
    "embed_dict[\"talk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999f859-fecd-434f-88e9-a54e02c88ab7",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "We don't need to consider the entire GloVe embedding weights, which has a shape of (400000, 100). Instead, if we aim to solve the task using only the _Zen of Python_ corpus, we can simply focus on its vocabulary. This will allow us to obtain an array with a shape of (82, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc60117-8223-4282-b45c-c2e3c0fa5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 80 words (2 misses)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# making a vocabulary disctionary from corpus:\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "# prepare embedding weights array:\n",
    "embedding_weights = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "for word, i in vocab_dict.items():\n",
    "    embedding_vector = embed_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        '''\n",
    "        Words that are not in the embedding index will be all zeros. \n",
    "        This also applies to the representations for \"padding\" and \n",
    "        \"out of vocabulary (OOV).\" \n",
    "        '''\n",
    "        embedding_weights[i] = embedding_vector\n",
    "        hits += 1\n",
    "\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f\"Converted {hits} words ({misses} misses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cb9162-797e-4009-8ccf-42dc8537e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights array:(82, 100)\n"
     ]
    }
   ],
   "source": [
    "# defining pretrained word-embedding:\n",
    "\n",
    "embedding = layers.Embedding(input_dim=vocab_size, \n",
    "                             output_dim=embed_dim,\n",
    "                             weights=[embedding_weights],\n",
    "                             trainable=False)\n",
    "\n",
    "'''\n",
    "Since we are using a pretrained weights, we don't want to lose\n",
    "them during the NN training. So, for embedding weights we set\n",
    "trainable=False.\n",
    "'''\n",
    "\n",
    "print(f\"weights array:{embedding.weights[0].shape}\")\n",
    "\n",
    "# get the embedded tokens:\n",
    "embed_tokens = embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f56ae6-51b6-4c0d-8721-8a3bf9878ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence:\n",
      "Beautiful is better than ugly\n",
      "\n",
      "- Token IDs:\n",
      "[74  2  4  3 28  0  0]\n",
      "\n",
      "- Embedded tokes:\n",
      "[[-0.18173    0.49759    0.46326    0.22507    0.46379    0.70062\n",
      "  -0.55155    0.79148   -0.18582    0.19755    0.19881    0.09037\n",
      "   0.02684    0.036921   0.25217    0.30879    0.33164    0.2714\n",
      "  -0.12808    1.1721    -0.072969   0.34904    0.11161   -0.36056\n",
      "   0.59628    0.42417   -0.69904   -0.19768   -0.35599   -0.23141\n",
      "  -0.38503   -0.12665    0.77121   -0.37397    0.59642   -0.24416\n",
      "  -0.25387   -0.065911   0.21035   -0.83429    0.28604   -0.022707\n",
      "   0.06746    0.088804   0.23424    0.20475    0.085396   0.55393\n",
      "   0.34153   -0.095455  -0.19291   -0.55262    1.0229     0.3866\n",
      "  -0.24254   -2.3519     0.43561    1.1172     0.77358   -0.73769\n",
      "  -0.35302    1.6699    -0.63955   -0.39244    0.56454   -0.27873\n",
      "   0.9252    -0.13997   -0.096213  -1.1242     0.49031    0.36918\n",
      "   0.41195   -0.038159   0.84123    0.24619    0.081767   0.07483\n",
      "   0.44646   -0.19423    0.013369   0.37712    0.23276    0.25728\n",
      "  -0.85934   -0.36652   -0.060819  -0.4635    -0.21186   -0.50654\n",
      "   0.33397   -0.24091    0.5626    -0.0414    -1.0032     0.1337\n",
      "  -1.8932    -0.81877   -0.44116    0.51389  ]\n",
      " [-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      "  -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "   0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "   0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "   0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      "  -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "   0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "   0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      "  -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      "  -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "   0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "   0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      "  -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      "  -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      "  -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      "  -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      "  -0.044219  -1.2969     0.76217    0.46349  ]\n",
      " [-0.047543   0.51914    0.34284   -0.09606   -0.4474    -0.3707\n",
      "  -0.12871   -0.50328   -0.26129   -0.090832  -0.060988  -0.36865\n",
      "   0.21908   -0.35645    0.32993   -0.29609   -0.018273   0.16066\n",
      "  -0.35906    0.67961    0.13921    0.12728   -0.097452  -0.15845\n",
      "  -0.24286   -0.26502   -0.41235   -1.0086    -0.055266   0.051596\n",
      "  -0.24647    0.69692   -0.010224  -0.14127    0.95922    0.40876\n",
      "  -0.54785    0.3935    -0.090709  -0.22418    0.0491    -0.34819\n",
      "  -0.044169  -0.42278   -0.63473    0.070979   0.13305   -0.5402\n",
      "  -0.013333  -1.6006    -0.39543   -0.17326   -0.23691    1.3752\n",
      "   0.20951   -2.4743     0.48555    0.20272    1.5026    -0.11918\n",
      "  -0.29868    0.6899    -0.87974   -0.041267   0.58979    0.13067\n",
      "   0.30467    0.3365     0.21907   -0.17314    0.22645   -0.18273\n",
      "   0.15126   -0.44416    0.66597    0.03561   -0.36268   -0.2546\n",
      "  -0.062423  -0.13725    0.53822    0.22712   -1.0619     0.1651\n",
      "  -1.8325     0.17062    0.18547   -0.15742   -0.83444   -0.32558\n",
      "   0.41771   -0.31883    0.1094    -0.43584   -0.5451     0.074827\n",
      "  -0.1703    -0.26859    0.48665    0.55609  ]\n",
      " [ 0.10305    1.2472     0.56724   -0.19117   -0.33626    0.43446\n",
      "   0.13137    0.10865   -0.30365    0.27153    0.28143   -0.070029\n",
      "   0.13069   -0.55749    0.65366   -0.73591   -0.28843    0.025475\n",
      "   0.045126   0.81985    1.1737    -0.023528   0.045885   0.081232\n",
      "   0.23694   -0.43013   -0.15377   -0.63078   -0.010964  -0.4923\n",
      "   0.40203    0.21917    0.060061  -0.4783     0.22343    0.61249\n",
      "  -0.1794     0.42189   -0.47547    0.23102   -0.42927   -0.67041\n",
      "   0.13216   -0.10817   -0.23242   -0.13758    0.60007   -1.0089\n",
      "  -0.23983   -0.79421    0.209     -0.6083    -0.05907    1.5629\n",
      "  -0.24259   -2.4183    -0.09008   -0.39131    2.014      0.62688\n",
      "   0.20731    1.137     -0.2814     0.067306   0.40942   -0.080047\n",
      "   0.66897    0.59008    0.80932   -0.32619   -0.10338   -0.44707\n",
      "  -0.20956    0.2248     0.036496  -0.12258    0.021003  -0.099487\n",
      "  -0.88655   -0.097359   0.89843    0.12676   -0.70422    0.29163\n",
      "  -1.4439     0.35709    0.48294   -0.044056  -0.021766  -0.47151\n",
      "   0.50925    0.11205   -0.17628   -0.50906   -0.9412    -0.05368\n",
      "  -0.57045   -0.25835    0.81651    0.089829 ]\n",
      " [ 0.26582    0.031967   0.44848   -0.9018    -0.94648    0.34442\n",
      "  -0.29809    0.291      0.52952   -0.087822   0.047245  -0.066301\n",
      "   0.20218    0.20225    0.7133     1.0266    -0.55837    0.11656\n",
      "   0.47346    0.37375    0.6703     0.29351   -0.024912   0.070651\n",
      "   0.57491   -0.12923   -0.31424    0.14851   -0.052807   0.16968\n",
      "   0.27143    0.073211   0.58492   -0.49478   -0.46697   -0.46938\n",
      "  -0.53379   -0.48424    0.62557    0.021208   0.08845   -0.18254\n",
      "  -0.26787   -0.72378   -0.51485    0.40463    0.40184    0.87136\n",
      "  -0.33877   -0.35486    0.05475   -0.45031    0.016127   0.82024\n",
      "  -0.053093  -1.6258     0.53884    0.91028    0.24317   -0.4293\n",
      "  -0.45226    0.719     -0.77261   -0.63545    0.78705   -0.58804\n",
      "   0.65244   -0.2961    -0.59294   -0.48271    0.20754   -0.1759\n",
      "  -0.29636    0.41624    0.17611    0.34145   -0.49378    0.3136\n",
      "  -0.45327    0.28334   -0.039159   0.07691    0.39651    0.73837\n",
      "  -0.91206   -0.77813   -0.76238   -0.032103  -0.13293   -0.36559\n",
      "  -0.10129    0.12534   -0.28914    0.057655   0.18079   -0.51944\n",
      "  -0.76692    0.21321   -0.098127   0.28455  ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "\n",
    "i = 0\n",
    "print(f\"- Sentence:\\n{corpus[i]}\\n\")\n",
    "print(f\"- Token IDs:\\n{token_ids[i]}\\n\")\n",
    "print(f\"- Embedded tokes:\\n{embed_tokens[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090efcf7-b3eb-42be-af01-0ca748818042",
   "metadata": {},
   "source": [
    "### __Reference:__\n",
    "<font size=3>\n",
    "    \n",
    " - [Deep Learning with Python](https://books.google.com.br/books/about/Deep_Learning_with_Python.html?id=Yo3CAQAACAAJ&redir_esc=y);\n",
    " - [Build a Large Language Model From Scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb);\n",
    " - [Understanding word-embedding with Keras](https://medium.com/@hsinhungw/understanding-word-embeddings-with-keras-dfafde0d15a4).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
