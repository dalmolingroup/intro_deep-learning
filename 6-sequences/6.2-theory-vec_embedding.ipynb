{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9ebf481-5eb7-443e-a5c2-45d8e1c0051f",
   "metadata": {},
   "source": [
    "### __3. Word-Embedding:__\n",
    "<font size=3>\n",
    "\n",
    "Word-embeddings represent words or tokens as dense vectors composed of float numbers, allowing for significantly lower-dimensional representations compared to one-hot encoding. These vectors are either learned during training or derived from pre-trained embeddings, which are especially useful for smaller datasets. \n",
    "\n",
    "For example, a word represented in one-hot encoding as [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] could have an embedding form like [0.245 -0.183 0.834]. The length of the one-hot vector corresponds to the vocabulary size ($\\mathtt{vocab\\_size}$), while the length of the embedding vector is determined by the embedding-dimension ($\\mathtt{embed\\_dim}$), a predefined hyperparameter.\n",
    "\n",
    "#### __3.1 How it works in practice:__\n",
    "    \n",
    "- We import the corpus;\n",
    "- Transform each sentence into a list of token IDs;\n",
    "- And make the word-embedding.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9575eeb8-7472-4873-bc6f-2b13af02724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:03:07.699792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 15:03:08.280562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21239d5d-8626-4f50-868e-60e948840a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import corpus:\n",
    "corpus = [\"Beautiful is better than ugly\",\n",
    "          \"Explicit is better than implicit\",\n",
    "          \"Simple is better than complex\",\n",
    "          \"Complex is better than complicated\",\n",
    "          \"Flat is better than nested\",\n",
    "          \"Sparse is better than dense\",\n",
    "          \"Readability counts\",\n",
    "          \"Special cases aren't special enough to break the rules\",\n",
    "          \"Although practicality beats purity\",\n",
    "          \"Errors should never pass silently\",\n",
    "          \"Unless explicitly silenced\",\n",
    "          \"In the face of ambiguity, refuse the temptation to guess\",\n",
    "          \"There should be one -- and preferably only one -- obvious way to do it\",\n",
    "          \"Although that way may not be obvious at first unless you're Dutch\",\n",
    "          \"Now is better than never\",\n",
    "          \"Although never is often better than right now\",\n",
    "          \"If the implementation is hard to explain, it's a bad idea\",\n",
    "          \"If the implementation is easy to explain, it may be a good idea\",\n",
    "          \"Namespaces are one honking great idea -- let's do more of those!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e86114c-0e3d-411a-8cab-15a64fa0bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82\n",
      "Vocabulary tokens: ['', '[UNK]', 'is', 'than', 'better', 'to', 'the', 'one', 'never', 'idea', 'be', 'although', 'way', 'unless', 'special', 'should', 'of', 'obvious', 'now', 'may', 'it', 'implementation', 'if', 'explain', 'do', 'complex', 'a', 'youre', 'ugly', 'those', 'there', 'that', 'temptation', 'sparse', 'simple', 'silently', 'silenced', 'rules', 'right', 'refuse', 'readability', 'purity', 'preferably', 'practicality', 'pass', 'only', 'often', 'not', 'nested', 'namespaces', 'more', 'lets', 'its', 'in', 'implicit', 'honking', 'hard', 'guess', 'great', 'good', 'flat', 'first', 'face', 'explicitly', 'explicit', 'errors', 'enough', 'easy', 'dutch', 'dense', 'counts', 'complicated', 'cases', 'break', 'beautiful', 'beats', 'bad', 'at', 'arent', 'are', 'and', 'ambiguity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:03:09.935375: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# 2. transform each sentence into a list of token IDs:\n",
    "\n",
    "vocab_size = None # maximum vocabulary size\n",
    "max_len = 7 # maximum sentence length\n",
    "\n",
    "vectorize = layers.TextVectorization(max_tokens=vocab_size,\n",
    "                                    standardize='lower_and_strip_punctuation',\n",
    "                                    split='whitespace',\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_len)\n",
    "\n",
    "vectorize.adapt(corpus)\n",
    "\n",
    "vocab = vectorize.get_vocabulary()\n",
    "vocab_size = vectorize.vocabulary_size()\n",
    "\n",
    "# get list of token IDs:\n",
    "token_ids = vectorize(corpus)\n",
    "\n",
    "# [UNK] = unknown word\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Vocabulary tokens:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8713ab-52f7-4f89-a782-e04b0c62a7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76004283e-02,  1.18379220e-02, -3.82632501e-02],\n",
       "       [-1.61314383e-02,  4.97332104e-02,  4.45997715e-03],\n",
       "       [-4.22321074e-02, -3.81650776e-03, -2.47380491e-02],\n",
       "       [-4.83655334e-02,  4.75725420e-02, -2.06314214e-02],\n",
       "       [ 4.75475825e-02,  1.04051828e-02, -6.49937242e-03],\n",
       "       [-8.57971981e-03,  1.35271661e-02,  1.59049295e-02],\n",
       "       [-4.14826982e-02,  4.78745587e-02,  4.92272042e-02],\n",
       "       [ 4.12114151e-02, -2.32710969e-02,  3.72593515e-02],\n",
       "       [ 1.47008039e-02,  1.12207048e-02,  3.70717384e-02],\n",
       "       [ 3.44085805e-02, -1.95440650e-02,  2.57262029e-02],\n",
       "       [-3.07475086e-02,  4.83743288e-02, -3.38538662e-02],\n",
       "       [-3.05345785e-02, -3.25664431e-02,  2.22492479e-02],\n",
       "       [-1.02776773e-02,  1.62848085e-03,  8.10660422e-04],\n",
       "       [ 2.46153027e-03, -4.31027897e-02, -2.20025666e-02],\n",
       "       [-4.01914828e-02, -2.46521831e-02, -2.58050207e-02],\n",
       "       [ 1.89334638e-02, -3.14366966e-02,  5.50774485e-03],\n",
       "       [-8.19776207e-03,  3.55802067e-02,  1.99849531e-03],\n",
       "       [-1.87994130e-02, -4.82260250e-02,  2.48417892e-02],\n",
       "       [ 1.12123974e-02,  1.75251812e-03, -2.34087557e-03],\n",
       "       [ 3.01198848e-02, -2.96054129e-02, -3.26007977e-02],\n",
       "       [ 4.06386964e-02,  2.16343515e-02,  3.83563302e-02],\n",
       "       [-2.35448480e-02, -2.65425332e-02,  2.39785574e-02],\n",
       "       [-1.38967410e-02, -4.02496234e-02, -2.82888487e-03],\n",
       "       [ 4.52170260e-02, -3.51953618e-02,  2.23374031e-02],\n",
       "       [ 3.56843583e-02,  1.22280233e-02, -4.89215925e-03],\n",
       "       [-2.86029465e-02,  5.82691282e-03,  3.01825143e-02],\n",
       "       [ 4.48811688e-02,  4.15404476e-02, -1.59117207e-02],\n",
       "       [-3.64371650e-02, -9.50377062e-03,  1.02800131e-02],\n",
       "       [ 1.04892366e-02, -1.51278265e-02,  4.02336009e-02],\n",
       "       [-4.71009500e-02, -4.31473367e-02,  2.48058476e-02],\n",
       "       [-7.99298286e-03,  2.62949206e-02,  1.07303634e-02],\n",
       "       [ 3.84761207e-02,  3.32386978e-02,  2.00180896e-02],\n",
       "       [-2.72798426e-02,  3.52439322e-02,  4.01129238e-02],\n",
       "       [ 5.09120524e-04,  1.65669993e-03, -3.36209424e-02],\n",
       "       [-5.72460890e-03, -1.39660463e-02,  7.29978085e-04],\n",
       "       [-3.16916779e-03, -1.90243479e-02,  1.61349773e-04],\n",
       "       [ 4.26327698e-02, -1.29306316e-03,  2.61282437e-02],\n",
       "       [-1.50749795e-02,  4.29188497e-02,  2.32875235e-02],\n",
       "       [ 3.25256474e-02, -3.86215672e-02,  3.49846594e-02],\n",
       "       [-3.60805281e-02,  1.38642304e-02,  2.25294568e-02],\n",
       "       [-3.73291150e-02, -1.05735660e-02, -7.73950666e-03],\n",
       "       [ 4.95484360e-02,  3.60963084e-02,  3.82450707e-02],\n",
       "       [ 4.59898971e-02, -3.29013839e-02, -1.56845227e-02],\n",
       "       [ 4.09220718e-02, -6.08333200e-03, -3.83044593e-02],\n",
       "       [-3.33260447e-02,  4.50677015e-02,  2.09584832e-05],\n",
       "       [ 3.89500596e-02, -2.10405514e-03,  2.09300183e-02],\n",
       "       [-4.37156446e-02, -1.08582377e-02, -1.64310113e-02],\n",
       "       [-1.51587017e-02, -3.36408615e-04, -1.28163584e-02],\n",
       "       [-4.24980409e-02, -6.32326677e-03,  1.99390166e-02],\n",
       "       [-3.41736302e-02,  3.33259441e-02, -1.52565837e-02],\n",
       "       [ 4.28233780e-02,  1.03387237e-02,  3.16306241e-02],\n",
       "       [-3.53658311e-02,  1.09452233e-02, -8.37960094e-03],\n",
       "       [-4.50850241e-02, -2.85033584e-02,  4.75687273e-02],\n",
       "       [-4.50186729e-02,  3.04874443e-02, -3.36814076e-02],\n",
       "       [ 2.35035755e-02, -2.18127966e-02, -4.50093523e-02],\n",
       "       [ 2.26372369e-02,  1.60898454e-02, -1.94931515e-02],\n",
       "       [-1.11724958e-02,  3.31956632e-02,  4.04187106e-02],\n",
       "       [ 3.70101817e-02, -2.21543666e-02, -4.46131080e-03],\n",
       "       [-1.69033408e-02,  1.28401034e-02,  2.74188258e-02],\n",
       "       [-4.74953540e-02,  3.83716822e-03, -1.53421052e-02],\n",
       "       [-2.36172210e-02,  3.83891724e-02, -6.82063028e-03],\n",
       "       [ 1.51136033e-02,  3.76554616e-02,  3.09361704e-02],\n",
       "       [ 1.13031268e-02,  4.02336381e-02, -2.70061977e-02],\n",
       "       [ 4.92295288e-02, -1.95798278e-02,  2.70355977e-02],\n",
       "       [ 3.17747928e-02,  1.88783519e-02,  2.21167877e-03],\n",
       "       [ 3.77826579e-02, -3.72519493e-02,  1.62418969e-02],\n",
       "       [ 4.49957289e-02, -8.42481852e-03,  1.67108215e-02],\n",
       "       [ 4.83925380e-02,  3.37817520e-03,  4.50782441e-02],\n",
       "       [-7.56703317e-04, -3.98934707e-02, -2.91027669e-02],\n",
       "       [-2.24616062e-02, -2.44183298e-02,  5.97209856e-03],\n",
       "       [ 4.81756963e-02, -1.03029497e-02, -4.10798192e-02],\n",
       "       [-4.28869389e-02,  3.83250378e-02, -2.91482937e-02],\n",
       "       [-1.14716887e-02,  4.97331470e-03,  3.05361785e-02],\n",
       "       [-2.06662547e-02, -1.78339258e-02, -4.68825586e-02],\n",
       "       [-5.86389378e-03, -2.08210479e-02, -4.45562601e-02],\n",
       "       [ 3.81675400e-02, -3.00171506e-02,  2.70878784e-02],\n",
       "       [ 2.53557228e-02, -4.86112125e-02, -9.12522152e-03],\n",
       "       [ 3.48180421e-02,  2.20360793e-02,  1.42572559e-02],\n",
       "       [ 2.48830654e-02, -1.25072487e-02,  2.62092389e-02],\n",
       "       [-4.55647372e-02, -3.63608003e-02, -4.00464050e-02],\n",
       "       [ 2.18774565e-02,  1.84711069e-03, -4.74616662e-02],\n",
       "       [-4.97474782e-02, -4.68261726e-02,  3.13158743e-02]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. make word-embedding:\n",
    "\n",
    "embed_dim = 3\n",
    "embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "# define input shape to initialize embedding weights:\n",
    "embedding.build(input_shape=(token_ids.shape))\n",
    "\n",
    "# print embedding weights - an array of shape (vocab_size, embed_dim):\n",
    "embedding.weights[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1d0c36-f413-45a2-8f6c-07fe4df9cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedded tokens\n",
    "embed_tokens = embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91b029c-c91d-451f-b75f-92042195044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence:\n",
      "Beautiful is better than ugly\n",
      "\n",
      "- Token IDs:\n",
      "[74  2  4  3 28  0  0]\n",
      "\n",
      "- Embedded tokes:\n",
      "[[-0.00586389 -0.02082105 -0.04455626]\n",
      " [-0.04223211 -0.00381651 -0.02473805]\n",
      " [ 0.04754758  0.01040518 -0.00649937]\n",
      " [-0.04836553  0.04757254 -0.02063142]\n",
      " [ 0.01048924 -0.01512783  0.0402336 ]\n",
      " [ 0.01760043  0.01183792 -0.03826325]\n",
      " [ 0.01760043  0.01183792 -0.03826325]]\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "\n",
    "i = 0\n",
    "print(f\"- Sentence:\\n{corpus[i]}\\n\")\n",
    "print(f\"- Token IDs:\\n{token_ids[i]}\\n\")\n",
    "print(f\"- Embedded tokes:\\n{embed_tokens[0].numpy()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5f7e47c-007d-4258-927a-e1cf85886ddf",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Note above that the token IDs __are__ the embedding weights row index! So each word is mapped by a vector of size $\\mathtt{embed\\_dim}$.\n",
    "\n",
    "#### __3.2 Pretrained word-embedding:__\n",
    "\n",
    "When working with a small dataset or aiming to reduce training computational costs, pretrained word embeddings can be highly beneficial. Popular [pretrained word-embedding](https://keras.io/examples/nlp/pretrained_word_embeddings/) include [Word2vec](https://code.google.com/archive/p/word2vec) and [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove). In this example, we will vectorize our _Zen of Python_ corpus using GloVe embeddings. \n",
    "\n",
    "To get started, we need to download the GloVe dataset using the cell below. The downloaded zip file contains embeddings with four different dimensional representations (50D, 100D, 200D, and 300D). For this task, we will focus on $\\mathtt{embed\\_dim=100}$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c1f073a-77fc-4b2d-a7d8-f5e5adeeddc7",
   "metadata": {},
   "source": [
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6960a321-696b-4402-8a15-261958886a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-7.9761e-02,  1.9551e-01,  3.0579e-01, -2.1571e-01, -4.9017e-01,\n",
       "        4.6350e-01, -1.5171e-01, -1.6002e-01,  1.3081e-01, -6.5718e-01,\n",
       "       -1.1343e-01,  1.0231e-01,  1.1583e-01,  2.0241e-03,  1.8107e-01,\n",
       "       -1.8263e-01, -4.2386e-01,  5.6726e-02, -3.0419e-01,  1.5828e-01,\n",
       "       -1.1820e-01,  1.8624e-01, -5.2731e-01, -5.9154e-01,  7.1546e-02,\n",
       "        1.9633e-01, -4.9147e-02, -3.3004e-01,  5.0489e-01,  5.1138e-01,\n",
       "       -5.0726e-01,  7.9255e-01,  1.7890e-01,  3.5001e-01, -7.2015e-02,\n",
       "        8.9293e-01, -2.7286e-01, -5.7761e-01,  1.8615e-01, -9.8489e-02,\n",
       "       -6.1398e-01,  6.1104e-02, -3.3847e-01, -2.9190e-01, -7.1794e-01,\n",
       "       -3.7329e-01, -3.2193e-01, -3.8184e-01,  4.9009e-02, -1.2856e+00,\n",
       "        3.1266e-02,  1.2953e-01,  1.1391e-01,  6.9458e-01,  3.3839e-01,\n",
       "       -2.1965e+00,  8.4632e-02,  7.6947e-02,  9.7508e-01,  3.2743e-01,\n",
       "        2.8664e-01,  7.9778e-01, -4.9729e-01, -1.1200e+00,  9.1580e-01,\n",
       "        8.9064e-02,  1.1378e+00,  3.3187e-01, -1.8245e-01,  1.7541e-01,\n",
       "        9.8961e-02, -3.9566e-01, -4.1590e-01, -7.4777e-01, -4.6913e-01,\n",
       "        3.8674e-01,  2.7161e-01, -1.5303e-02, -5.2653e-01, -2.0984e-01,\n",
       "        1.2046e-01, -4.0667e-01,  2.9756e-01, -1.3695e-01, -1.3846e+00,\n",
       "       -1.4904e-01, -4.8938e-01,  7.5170e-01, -2.2143e-01, -5.2081e-01,\n",
       "        2.6477e-01,  2.7790e-01, -4.1847e-01, -2.1104e-01, -7.4714e-01,\n",
       "       -6.5609e-02, -2.6370e-01,  2.0017e-01,  8.7429e-01,  6.9208e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get token vectors:\n",
    "embed_dim = 100\n",
    "embed_dict = {}\n",
    "\n",
    "with open(f\"../dataset/glove.6B.{embed_dim}d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embed_dict[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embed_dict)} word vectors.\")\n",
    "\n",
    "embed_dict[\"talk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999f859-fecd-434f-88e9-a54e02c88ab7",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "We don't need to consider the entire GloVe embedding weights, which has a shape of (400000, 100). Instead, if we aim to solve the task using only the _Zen of Python_ corpus, we can simply focus on its vocabulary. This will allow us to obtain an array with a shape of (82, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc60117-8223-4282-b45c-c2e3c0fa5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 80 words (2 misses)\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# making a vocabulary disctionary from corpus:\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "# prepare embedding weights array:\n",
    "embedding_weights = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "for word, i in vocab_dict.items():\n",
    "    embedding_vector = embed_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        '''\n",
    "        Words that are not in the embedding index will be all zeros. \n",
    "        This also applies to the representations for \"padding\" and \n",
    "        \"out of vocabulary (OOV).\" \n",
    "        '''\n",
    "        embedding_weights[i] = embedding_vector\n",
    "        hits += 1\n",
    "\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f\"Converted {hits} words ({misses} misses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cb9162-797e-4009-8ccf-42dc8537e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights array:(82, 100)\n"
     ]
    }
   ],
   "source": [
    "# defining pretrained word-embedding:\n",
    "\n",
    "embedding = layers.Embedding(input_dim=vocab_size, \n",
    "                             output_dim=embed_dim,\n",
    "                             weights=[embedding_weights],\n",
    "                             trainable=False)\n",
    "\n",
    "'''\n",
    "Since we are using a pretrained weights, we don't want to lose\n",
    "them during the NN training. So, for embedding weights we set\n",
    "trainable=False.\n",
    "'''\n",
    "\n",
    "print(f\"weights array:{embedding.weights[0].shape}\")\n",
    "\n",
    "# get the embedded tokens:\n",
    "embed_tokens = embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f56ae6-51b6-4c0d-8721-8a3bf9878ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentence:\n",
      "Beautiful is better than ugly\n",
      "\n",
      "- Token IDs:\n",
      "[74  2  4  3 28  0  0]\n",
      "\n",
      "- Embedded tokes:\n",
      "[[-0.18173    0.49759    0.46326    0.22507    0.46379    0.70062\n",
      "  -0.55155    0.79148   -0.18582    0.19755    0.19881    0.09037\n",
      "   0.02684    0.036921   0.25217    0.30879    0.33164    0.2714\n",
      "  -0.12808    1.1721    -0.072969   0.34904    0.11161   -0.36056\n",
      "   0.59628    0.42417   -0.69904   -0.19768   -0.35599   -0.23141\n",
      "  -0.38503   -0.12665    0.77121   -0.37397    0.59642   -0.24416\n",
      "  -0.25387   -0.065911   0.21035   -0.83429    0.28604   -0.022707\n",
      "   0.06746    0.088804   0.23424    0.20475    0.085396   0.55393\n",
      "   0.34153   -0.095455  -0.19291   -0.55262    1.0229     0.3866\n",
      "  -0.24254   -2.3519     0.43561    1.1172     0.77358   -0.73769\n",
      "  -0.35302    1.6699    -0.63955   -0.39244    0.56454   -0.27873\n",
      "   0.9252    -0.13997   -0.096213  -1.1242     0.49031    0.36918\n",
      "   0.41195   -0.038159   0.84123    0.24619    0.081767   0.07483\n",
      "   0.44646   -0.19423    0.013369   0.37712    0.23276    0.25728\n",
      "  -0.85934   -0.36652   -0.060819  -0.4635    -0.21186   -0.50654\n",
      "   0.33397   -0.24091    0.5626    -0.0414    -1.0032     0.1337\n",
      "  -1.8932    -0.81877   -0.44116    0.51389  ]\n",
      " [-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      "  -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "   0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "   0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "   0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      "  -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "   0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "   0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      "  -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      "  -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "   0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "   0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      "  -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      "  -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      "  -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      "  -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      "  -0.044219  -1.2969     0.76217    0.46349  ]\n",
      " [-0.047543   0.51914    0.34284   -0.09606   -0.4474    -0.3707\n",
      "  -0.12871   -0.50328   -0.26129   -0.090832  -0.060988  -0.36865\n",
      "   0.21908   -0.35645    0.32993   -0.29609   -0.018273   0.16066\n",
      "  -0.35906    0.67961    0.13921    0.12728   -0.097452  -0.15845\n",
      "  -0.24286   -0.26502   -0.41235   -1.0086    -0.055266   0.051596\n",
      "  -0.24647    0.69692   -0.010224  -0.14127    0.95922    0.40876\n",
      "  -0.54785    0.3935    -0.090709  -0.22418    0.0491    -0.34819\n",
      "  -0.044169  -0.42278   -0.63473    0.070979   0.13305   -0.5402\n",
      "  -0.013333  -1.6006    -0.39543   -0.17326   -0.23691    1.3752\n",
      "   0.20951   -2.4743     0.48555    0.20272    1.5026    -0.11918\n",
      "  -0.29868    0.6899    -0.87974   -0.041267   0.58979    0.13067\n",
      "   0.30467    0.3365     0.21907   -0.17314    0.22645   -0.18273\n",
      "   0.15126   -0.44416    0.66597    0.03561   -0.36268   -0.2546\n",
      "  -0.062423  -0.13725    0.53822    0.22712   -1.0619     0.1651\n",
      "  -1.8325     0.17062    0.18547   -0.15742   -0.83444   -0.32558\n",
      "   0.41771   -0.31883    0.1094    -0.43584   -0.5451     0.074827\n",
      "  -0.1703    -0.26859    0.48665    0.55609  ]\n",
      " [ 0.10305    1.2472     0.56724   -0.19117   -0.33626    0.43446\n",
      "   0.13137    0.10865   -0.30365    0.27153    0.28143   -0.070029\n",
      "   0.13069   -0.55749    0.65366   -0.73591   -0.28843    0.025475\n",
      "   0.045126   0.81985    1.1737    -0.023528   0.045885   0.081232\n",
      "   0.23694   -0.43013   -0.15377   -0.63078   -0.010964  -0.4923\n",
      "   0.40203    0.21917    0.060061  -0.4783     0.22343    0.61249\n",
      "  -0.1794     0.42189   -0.47547    0.23102   -0.42927   -0.67041\n",
      "   0.13216   -0.10817   -0.23242   -0.13758    0.60007   -1.0089\n",
      "  -0.23983   -0.79421    0.209     -0.6083    -0.05907    1.5629\n",
      "  -0.24259   -2.4183    -0.09008   -0.39131    2.014      0.62688\n",
      "   0.20731    1.137     -0.2814     0.067306   0.40942   -0.080047\n",
      "   0.66897    0.59008    0.80932   -0.32619   -0.10338   -0.44707\n",
      "  -0.20956    0.2248     0.036496  -0.12258    0.021003  -0.099487\n",
      "  -0.88655   -0.097359   0.89843    0.12676   -0.70422    0.29163\n",
      "  -1.4439     0.35709    0.48294   -0.044056  -0.021766  -0.47151\n",
      "   0.50925    0.11205   -0.17628   -0.50906   -0.9412    -0.05368\n",
      "  -0.57045   -0.25835    0.81651    0.089829 ]\n",
      " [ 0.26582    0.031967   0.44848   -0.9018    -0.94648    0.34442\n",
      "  -0.29809    0.291      0.52952   -0.087822   0.047245  -0.066301\n",
      "   0.20218    0.20225    0.7133     1.0266    -0.55837    0.11656\n",
      "   0.47346    0.37375    0.6703     0.29351   -0.024912   0.070651\n",
      "   0.57491   -0.12923   -0.31424    0.14851   -0.052807   0.16968\n",
      "   0.27143    0.073211   0.58492   -0.49478   -0.46697   -0.46938\n",
      "  -0.53379   -0.48424    0.62557    0.021208   0.08845   -0.18254\n",
      "  -0.26787   -0.72378   -0.51485    0.40463    0.40184    0.87136\n",
      "  -0.33877   -0.35486    0.05475   -0.45031    0.016127   0.82024\n",
      "  -0.053093  -1.6258     0.53884    0.91028    0.24317   -0.4293\n",
      "  -0.45226    0.719     -0.77261   -0.63545    0.78705   -0.58804\n",
      "   0.65244   -0.2961    -0.59294   -0.48271    0.20754   -0.1759\n",
      "  -0.29636    0.41624    0.17611    0.34145   -0.49378    0.3136\n",
      "  -0.45327    0.28334   -0.039159   0.07691    0.39651    0.73837\n",
      "  -0.91206   -0.77813   -0.76238   -0.032103  -0.13293   -0.36559\n",
      "  -0.10129    0.12534   -0.28914    0.057655   0.18079   -0.51944\n",
      "  -0.76692    0.21321   -0.098127   0.28455  ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# one sentence example:\n",
    "\n",
    "i = 0\n",
    "print(f\"- Sentence:\\n{corpus[i]}\\n\")\n",
    "print(f\"- Token IDs:\\n{token_ids[i]}\\n\")\n",
    "print(f\"- Embedded tokes:\\n{embed_tokens[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabfec9-4fcf-4dcb-bb5a-0dc3e10210d2",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "A key advantage of word embeddings is their ability to capture the semantic relationships between words in the embedding space. For instance, synonyms tend to have similar vector representations, resulting in closer geometric distances. This allows for intuitive relationships, such as \"dog\" being close to \"wolf\" and \"cat\" near \"tiger.\" Additionally, embeddings can model analogical reasoning, where adding specific vectors reflects meaningful transformations, such as \"king\" + $\\mathtt{female\\_vector}$ = \"queen\" or \"tree\" + $\\mathtt{plural\\_vector}$ = \"trees.\" This powerful ability to encode relationships makes word embeddings essential for many natural language processing tasks.\n",
    "\n",
    "This _\"geometric distance\"_ is typically measured using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), which calculates the dot product between two word-vectors, $\\;\\vec a\\cdot\\vec b = |\\vec a|\\,|\\vec b|\\,\\cos\\theta\\;$,\n",
    "$$\n",
    "    \\cos\\theta = \\dfrac{\\vec a\\cdot\\vec b}{|\\vec a||\\vec b|} \\, ,\n",
    "$$\n",
    "where $\\cos\\theta \\in [-1, +1]$, from opposite to proporcional vectors.\n",
    "\n",
    "The GloVe dataset includes words from various languages, meaning that some opposite correlations may arise from words in different languages. Below are some examples of cosine similarity using only English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99219513-f835-4528-a00e-ee6e5201b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(good vs nice): 0.7312453\n",
      "(ripened vs reaffirm): 0.00025181586\n",
      "(line-out vs court): -0.2146855\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \n",
    "    A = np.linalg.norm(a)\n",
    "    B = np.linalg.norm(b)\n",
    "\n",
    "    return np.dot(a, b)/(A*B)\n",
    "\n",
    "print(\"(good vs nice):\", cosine_similarity(embed_dict[\"good\"], embed_dict[\"nice\"]))\n",
    "print(\"(ripened vs reaffirm):\", cosine_similarity(embed_dict[\"ripened\"], embed_dict[\"reaffirm\"]))\n",
    "print(\"(line-out vs court):\", cosine_similarity(embed_dict[\"line-out\"], embed_dict[\"court\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05037806-c22b-41c6-badf-af12c529ab27",
   "metadata": {},
   "source": [
    "#### __3.3 [Extra] Linear _vs_ embedding layers:__\n",
    "<font size=3>\n",
    "\n",
    "It is said that embedding layers function similarly to linear layers. To gain a deeper understanding of how word embedding works, letâ€™s compare their similarities and differences.\n",
    "\n",
    "A linear embedding-like layer behaves like a linear layer, meaning it has no activation function (since it is linear) and does not include a bias vector. When considering the input as a one-hot vector $a_0^i$, the output of the embedding is given by:\n",
    "\\begin{align}\n",
    "     a_0^i\\;\\; W^{ij} &= a_1^j \\, ,\\\\\\\\\n",
    "     \\begin{pmatrix}\n",
    "     0 & 1 & 0 & 0\n",
    "     \\end{pmatrix}\n",
    "     \\begin{pmatrix}\n",
    "         w_{00} & w_{01} & w_{02} \\\\\n",
    "         w_{10} & w_{11} & w_{12} \\\\\n",
    "         w_{20} & w_{21} & w_{22} \\\\\n",
    "         w_{30} & w_{31} & w_{32} \n",
    "     \\end{pmatrix}\n",
    "     &=\n",
    "     \\begin{pmatrix}\n",
    "         w_{10} & w_{11} & w_{12}\n",
    "     \\end{pmatrix}  \\, ,\n",
    "\\end{align}\n",
    "where the indexes $\\; i \\in [0,\\, \\mathtt{vocab\\_size})$ and $\\; j \\in [0,\\, \\mathtt{embed\\_dim})$. However, an embedding layer acts like,\n",
    "\\begin{align}\n",
    "    \\delta^{ki}\\;\\;W^{ij} &= a_1^i\\, ,\\\\\\\\\n",
    "     \\begin{pmatrix}\n",
    "         w_{00} & w_{01} & w_{02} \\\\\n",
    "         [w_{10} & w_{11} & w_{12}] \\\\\n",
    "         w_{20} & w_{21} & w_{22} \\\\\n",
    "         w_{30} & w_{31} & w_{32} \n",
    "     \\end{pmatrix} \n",
    "      &\\Rightarrow\n",
    "     \\begin{pmatrix}\n",
    "         w_{10} & w_{11} & w_{12}\n",
    "     \\end{pmatrix} \\, ,\n",
    "\\end{align}\n",
    "where $\\delta^{kj} = [1,\\; k=j;\\quad 0,\\; k\\neq j]$, and $k$ is a $\\mathtt{token\\_id}$, so it selects the vector $(w_{10}\\;\\; w_{11}\\;\\;  w_{12})$ without one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63816f8-1766-4b9f-984c-d93cbd2a463e",
   "metadata": {},
   "source": [
    "#### __3.3.1 Linear layer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db3ae19-3270-42db-ba5c-5d8f068fce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider that the corpus text presents a vocabulary size of:\n",
    "vocab_size = 10\n",
    "\n",
    "# and wee want to embedding the corpus into matrix of dimension:\n",
    "embed_dim = 5\n",
    "\n",
    "# Also, we'll consider the following token-IDs list:\n",
    "token_ids = np.array([3, 8, 2, 5, 0])\n",
    "\n",
    "''' Note that the ID = 0 in token_ids represent the padding.\n",
    "To compare linear vs embedding layer, the input-layer size (max-len)\n",
    "need to have the same size as embed_im.'''\n",
    "\n",
    "max_len = token_ids.size\n",
    "\n",
    "assert max_len == embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fdbd42a-f290-4624-b9b6-d8db1f11d068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "8: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "2: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "5: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "0: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding from token-ids:\n",
    "onehot = layers.CategoryEncoding(num_tokens=vocab_size, output_mode=\"one_hot\")(token_ids)\n",
    "\n",
    "for ID, hot in zip(token_ids, onehot):\n",
    "    print(f\"{ID}: {hot}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5a14f8-7a5b-4826-9867-8aa1968ce465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Linear weights:\n",
      " [[ 0.49218947  0.36513233 -0.56878525 -0.26862016  0.49037963]\n",
      " [ 0.02909946 -0.43373728  0.35295624  0.40925092  0.6102509 ]\n",
      " [ 0.6071431   0.14801794 -0.33090723  0.05308819  0.09311652]\n",
      " [-0.14760238  0.15635943  0.08833092  0.3197629   0.00100005]\n",
      " [-0.39183617  0.13605869  0.5090255  -0.1358301  -0.34157965]\n",
      " [ 0.54999894  0.02152151 -0.19227457  0.35214847  0.17522681]\n",
      " [-0.25164026 -0.54330355 -0.13154769  0.06628239 -0.3393166 ]\n",
      " [-0.41437823 -0.39346197  0.13575953  0.5055633  -0.18641716]\n",
      " [ 0.31544185  0.61205226  0.03930789  0.5345184  -0.38107675]\n",
      " [ 0.23140794 -0.13635138 -0.20535493 -0.38744277 -0.17155147]]\n",
      "\n",
      "- Linear layer's outputs:\n",
      " [[-0.14760238  0.15635943  0.08833092  0.3197629   0.00100005]\n",
      " [ 0.31544185  0.61205226  0.03930789  0.5345184  -0.38107675]\n",
      " [ 0.6071431   0.14801794 -0.33090723  0.05308819  0.09311652]\n",
      " [ 0.54999894  0.02152151 -0.19227457  0.35214847  0.17522681]\n",
      " [ 0.49218947  0.36513233 -0.56878525 -0.26862016  0.49037963]]\n"
     ]
    }
   ],
   "source": [
    "# defining linear layer:\n",
    "linear = layers.Dense(units=max_len, bias_initializer=\"zeros\")\n",
    "linear.build(input_shape=onehot.shape) # initialize weights\n",
    "\n",
    "weights = linear.weights[0].numpy() \n",
    "\n",
    "print(\"- Linear weights:\\n\", weights, end=\"\\n\\n\")\n",
    "print(\"- Linear layer's outputs:\\n\", linear(onehot).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0afbcb4-d608-4ff3-9eb2-c4141cd55bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Dot product outputs:\n",
      " [[-0.14760238  0.15635943  0.08833092  0.3197629   0.00100005]\n",
      " [ 0.31544185  0.61205226  0.03930789  0.5345184  -0.38107675]\n",
      " [ 0.6071431   0.14801794 -0.33090723  0.05308819  0.09311652]\n",
      " [ 0.54999894  0.02152151 -0.19227457  0.35214847  0.17522681]\n",
      " [ 0.49218947  0.36513233 -0.56878525 -0.26862016  0.49037963]]\n"
     ]
    }
   ],
   "source": [
    "# using only the dot/inner product:\n",
    "print(\"- Dot product outputs:\\n\", np.dot(onehot, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc6c6e-d252-4d99-891f-6f5ec63dfbe5",
   "metadata": {},
   "source": [
    "#### __3.3.2 Embedding layer__\n",
    "<font size=3>\n",
    "    \n",
    "Here, the embedding layer selects $\\mathtt{weight}$ rows using $\\mathtt{token\\_ids}$, without the need for one-hot encoding matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f549a581-c14f-4715-ba60-0fb5ae954653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Embedding weights:\n",
      " [[ 0.49218947  0.36513233 -0.56878525 -0.26862016  0.49037963]\n",
      " [ 0.02909946 -0.43373728  0.35295624  0.40925092  0.6102509 ]\n",
      " [ 0.6071431   0.14801794 -0.33090723  0.05308819  0.09311652]\n",
      " [-0.14760238  0.15635943  0.08833092  0.3197629   0.00100005]\n",
      " [-0.39183617  0.13605869  0.5090255  -0.1358301  -0.34157965]\n",
      " [ 0.54999894  0.02152151 -0.19227457  0.35214847  0.17522681]\n",
      " [-0.25164026 -0.54330355 -0.13154769  0.06628239 -0.3393166 ]\n",
      " [-0.41437823 -0.39346197  0.13575953  0.5055633  -0.18641716]\n",
      " [ 0.31544185  0.61205226  0.03930789  0.5345184  -0.38107675]\n",
      " [ 0.23140794 -0.13635138 -0.20535493 -0.38744277 -0.17155147]]\n",
      "\n",
      "- Embedding layer's outputs:\n",
      " [[-0.14760238  0.15635943  0.08833092  0.3197629   0.00100005]\n",
      " [ 0.31544185  0.61205226  0.03930789  0.5345184  -0.38107675]\n",
      " [ 0.6071431   0.14801794 -0.33090723  0.05308819  0.09311652]\n",
      " [ 0.54999894  0.02152151 -0.19227457  0.35214847  0.17522681]\n",
      " [ 0.49218947  0.36513233 -0.56878525 -0.26862016  0.49037963]]\n"
     ]
    }
   ],
   "source": [
    "# defining the embedding layer using the same linear weights for comparison:\n",
    "embedding = layers.Embedding(input_dim=vocab_size, output_dim=max_len, weights=[weights])\n",
    "\n",
    "print(\"- Embedding weights:\\n\", embedding.weights[0].numpy(), end=\"\\n\\n\")\n",
    "print(\"- Embedding layer's outputs:\\n\", embedding(token_ids).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090efcf7-b3eb-42be-af01-0ca748818042",
   "metadata": {},
   "source": [
    "### __Reference:__\n",
    "<font size=3>\n",
    "    \n",
    " - [Deep Learning with Python](https://books.google.com.br/books/about/Deep_Learning_with_Python.html?id=Yo3CAQAACAAJ&redir_esc=y);\n",
    " - [Build a Large Language Model From Scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb);\n",
    " - [Understanding word-embedding with Keras](https://medium.com/@hsinhungw/understanding-word-embeddings-with-keras-dfafde0d15a4).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
