{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c3dd3-9597-4731-9051-340815c9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e02c34-a721-4728-a8f5-22efd2f5209e",
   "metadata": {},
   "source": [
    "### __1. Import and data pre-processing:__\n",
    "<font size=3>\n",
    "\n",
    "For our natural language processing task, we will focus on text classification in sentiment analysis using the [IMDB movie reviews](https://keras.io/api/datasets/imdb/) dataset. To download the text reviews, click [here](http://mng.bz/0tIo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa56b8-68e0-4f5d-b001-2f9a0b2ae6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the texts:\n",
    "\n",
    "'''\n",
    "The texts are separated into positive (25000) and negative reviews (25000).\n",
    "Let's take all together to split them into train, validation, and test arrays.\n",
    "'''\n",
    "\n",
    "path = \"../dataset/aclImdb/\"\n",
    "\n",
    "texts, labels = [], []\n",
    "\n",
    "for data in [\"train/\", \"test/\"]:\n",
    "    for label in [\"pos/\", \"neg/\"]:\n",
    "            \n",
    "        for file in os.listdir(path+data+label):\n",
    "            with open(path+data+label+file, \"r\") as f:\n",
    "                texts.append(f.read())\n",
    "                \n",
    "                if label == \"pos/\": labels.append(1)\n",
    "                else: labels.append(0)\n",
    "\n",
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9141d1-050f-426e-98cc-dda72f5ca46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the texts to obtain their words as indexes of the \"vocab\" array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e996e59-48fb-4783-b8fb-474d064c4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x, y) dataset:\n",
    "\n",
    "# shuffing the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922ba8b-ceae-40f6-8ad6-ce91cf4155c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data between train, validation and test:\n",
    "N_test = int(0.1*N_sentences)\n",
    "N_val = int(0.2*N_sentences)\n",
    "N_train = N_sentences - (N_val + N_test)\n",
    "\n",
    "x_train = x[:N_train]\n",
    "x_val = x[N_train:N_train+N_val]\n",
    "x_test = x[N_train+N_val:]\n",
    "\n",
    "print(f\"x-train:{x_train.shape}, x-val:{x_val.shape}, x-test:{x_test.shape}\")\n",
    "\n",
    "y_train = y[:N_train]\n",
    "y_val = y[N_train:N_train+N_val]\n",
    "y_test = y[N_train+N_val:]\n",
    "\n",
    "print(f\"y-train:{y_train.shape}, y-val:{y_val.shape}, y-test:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac43d1-c2c1-421e-8370-c76b1fa75ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting unnecessary tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297d543-bb47-455f-9cc3-0e129221f760",
   "metadata": {},
   "source": [
    "### __2. Neural network modeling:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35a7c4-956f-461b-bd3a-d0f171eb06a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dc2e5-f90e-45ea-85e7-c2f80af00c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626664fc-593b-4539-ac60-4903c5734b58",
   "metadata": {},
   "source": [
    "### __3. Model compilation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324e21e-c165-4fef-a672-98ae835c7461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e9c91-c639-42bf-b3f0-f0b07100b934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beda4521-0d6c-42fe-995f-dd52c4ae9bc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __4. Train and validation__   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "524166d4-af16-4e67-be28-17472ca460e0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "report = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val], batch_size=40, epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22ce8ca5-5add-438a-9d8e-dd8632fa62dd",
   "metadata": {},
   "source": [
    "loss = report.history['loss']\n",
    "val_loss = report.history['val_loss']\n",
    "\n",
    "acc = report.history['acc']\n",
    "val_acc = report.history['val_acc']\n",
    "\n",
    "epochs = np.linspace(1, len(loss), len(loss))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(epochs, loss, label=\"loss\")\n",
    "ax[0].plot(epochs, val_loss, label=\"val-loss\")\n",
    "\n",
    "ax[1].plot(epochs, acc, label=\"acc\")\n",
    "ax[1].plot(epochs, val_acc, label=\"val-acc\")\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8686e6-36cf-414e-8fed-238a7c2024ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __5. Final training__    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a25f2c1-f658-44d6-9b2c-bf6052ea56ef",
   "metadata": {},
   "source": [
    "report = model.fit(x=x_train, y=y_train, batch_size=40, epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4164d979-9e3d-4081-beff-67bcab024230",
   "metadata": {},
   "source": [
    "loss = report.history['loss']\n",
    "acc = report.history['acc']\n",
    "\n",
    "epochs = np.linspace(1, len(loss), len(loss))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(epochs, loss, label=\"loss\")\n",
    "ax[1].plot(epochs, acc, label=\"acc\")\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"epochs\")\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e875dc-7d57-42fb-a03b-19948e2de183",
   "metadata": {},
   "source": [
    "### __6. Test evaluation__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddccce0-d412-4e34-a5d6-26e442f7314c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6160d5-f5f9-42af-9cb3-a91cb6516774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1006c9-50c7-4341-a265-a9221ae1ebca",
   "metadata": {},
   "source": [
    "### __7. Saving the model__:\n",
    "<font size=3>\n",
    "    \n",
    "For model __loading__, see [2.3-handson-loading_model](../2-over_vs_underfitting/2.3-handson-loading_model.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
